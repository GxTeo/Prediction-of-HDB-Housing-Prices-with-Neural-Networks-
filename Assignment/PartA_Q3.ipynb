{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,ModelCheckpoint,LearningRateScheduler\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "import os\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "import random \n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./full.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['label','filename']\n",
    "\n",
    "def split_dataset(df, columns_to_drop, test_size, random_state):\n",
    "  label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "  df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "  df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "  df_train2 = df_train.drop(columns_to_drop,axis=1)\n",
    "  y_train2 = df_train['label'].to_numpy()\n",
    "\n",
    "  df_test2 = df_test.drop(columns_to_drop,axis=1)\n",
    "  y_test2 = df_test['label'].to_numpy() \n",
    "\n",
    "  return df_train2, y_train2, df_test2, y_test2\n",
    "\n",
    "def preprocess_dataset(df_train, df_test):\n",
    "\n",
    "  standard_scaler = preprocessing.StandardScaler()\n",
    "  df_train_scaled = standard_scaler.fit_transform(df_train)\n",
    "\n",
    "  df_test_scaled = standard_scaler.transform(df_test)\n",
    "\n",
    "  return df_train_scaled, df_test_scaled\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_dataset(df, columns_to_drop, test_size=0.3, random_state=0) # positive labels being encoded as 1\n",
    "\n",
    "X_train_scaled, X_test_scaled = preprocess_dataset(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 128\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "no_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Timing callback for every epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimingCallBack class for Q2b\n",
    "\n",
    "class TimingCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.times.append(time.time() - self.epoch_time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Callback for early stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of folds and batch sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_folds = 5\n",
    "cv = KFold(n_splits=no_folds, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neurons_list = [\"model_neurons_64\", \"model_neurons_128\", \"model_neurons_256\"]\n",
    "num_neurons_list = [64,128,256]\n",
    "\n",
    "Q3_X, Q3_Y = X_train_scaled, y_train\n",
    "neurons_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neurons_list = [\"model_neurons_64\", \"model_neurons_128\", \"model_neurons_256\"]\n",
    "num_neurons_list = [64,128,256]\n",
    "model_fold = [\"_0\", \"_1\", \"_2\", \"_3\" ,\"_4\"]\n",
    "optimal_batch_size = 128 #From Question 2\n",
    "\n",
    "Q3_X, Q3_Y = X_train_scaled, y_train\n",
    "neurons_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the optimal number of hidden neurons for the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_time_taken_dict = {}\n",
    "Q3_model_acc = {}\n",
    "Q3_model_train_acc = {}\n",
    "Q3_model_loss = {}\n",
    "Q3_history = {}\n",
    "for model_neurons in model_neurons_list:    \n",
    "    fold = 0\n",
    "    val_acc = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    time_taken_list = []\n",
    "    for train_idx, test_idx in cv.split(Q3_X, Q3_Y):\n",
    "        Q3_cb = TimingCallback()\n",
    "        Q3_X_train, Q3_y_train  = Q3_X[train_idx], Q3_Y[train_idx]\n",
    "        Q3_X_test, Q3_y_test = Q3_X[test_idx], Q3_Y[test_idx]\n",
    "        Q3_model = Sequential([Dense(num_neurons_list[neurons_idx], activation='relu'),\n",
    "                        Dropout(0.2), Dense(num_neurons, activation ='relu'),\n",
    "                        Dropout(0.2), Dense(num_neurons, activation='relu'),\n",
    "                        Dropout(0.2), Dense(1, activation='sigmoid')])\n",
    "        \n",
    "        Q3_model.compile(optimizer='adam', \n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "        \n",
    "        Q3_history[model_neurons + model_fold[fold]] = Q3_model.fit(Q3_X_train, Q3_y_train,\n",
    "                        batch_size = optimal_batch_size,\n",
    "                        epochs=no_epochs,\n",
    "                        verbose=1,\n",
    "                        use_multiprocessing=True,\n",
    "                        validation_data=(Q3_X_test, Q3_y_test), callbacks=[callback, Q3_cb])\n",
    "        \n",
    "        #Time taken of final epoch for each fold \n",
    "        time_taken_list.append(Q3_cb.times[-1])\n",
    "        #print(\"Time Taken for final epoch \" + model_neurons + model_fold[fold] + \": {}\".format(Q3_cb.times[-1]))\n",
    "\n",
    "        #Val accuracy of final epoch of each fold\n",
    "        val_acc.append(Q3_history[model_neurons + model_fold[fold]].history['val_accuracy'][-1])\n",
    "\n",
    "        #Train accuracy of final epoch of each fold\n",
    "        train_acc.append(Q3_history[model_neurons + model_fold[fold]].history['accuracy'][-1])\n",
    "\n",
    "        #Val loss of final epoch of each fold\n",
    "        val_loss.append(Q3_history[model_neurons + model_fold[fold]].history['val_loss'][-1])\n",
    "        \n",
    "        #print(model_neurons +' fold %d test accuracy %g'%(fold, val_acc[fold]))\n",
    "        fold += 1\n",
    "    \n",
    "    Q3_model_acc[model_neurons] = val_acc\n",
    "    Q3_model_train_acc[model_neurons] = train_acc\n",
    "    Q3_time_taken_dict[model_neurons]= time_taken_list\n",
    "    Q3_model_loss[model_neurons] = val_loss\n",
    "    print(model_neurons + '* mean accuracy = %g *'% np.mean(val_acc))\n",
    "    neurons_idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean cross-validation accuracies on the final epoch for different numbers of hidden-layer neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_mean_val_acc = []\n",
    "for key, value in Q3_model_acc.items():\n",
    "    Q3_mean_val_acc.append(np.mean(value))\n",
    "\n",
    "Q3_mean_val_loss = []\n",
    "for key, value in Q3_model_loss.items():\n",
    "    Q3_mean_val_loss.append(np.mean(value))\n",
    "\n",
    "plt_1 = plt.figure(figsize=(15, 10))\n",
    "plt.scatter(num_neurons_list, Q3_mean_val_acc, marker = 'X')\n",
    "plt.title('Mean Cross-Validation Accuracy vs Number of neurons for optimal batch size: {}'.format(optimal_batch_size))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('No. of neurons')\n",
    "plt.xticks(num_neurons_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3_mean_time_taken = []\n",
    "for key, value in Q3_time_taken_dict.items():\n",
    "    Q3_mean_time_taken.append(np.mean(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Time Taken on the final epoch for different numbers of hidden-layer neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_1 = plt.figure(figsize=(15, 10))\n",
    "plt.scatter(num_neurons_list, Q3_mean_time_taken, marker = 'x')  \n",
    "plt.title('Mean Time Taken vs No.of neurons for optimal batch size: {}'.format(optimal_batch_size))\n",
    "plt.ylabel('Mean Time Taken')\n",
    "plt.xlabel('No.of neurons')\n",
    "plt.xticks(num_neurons_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3B \n",
    "#### Select the optimal number of neurons for the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = pd.DataFrame.from_dict(Q3_model_acc,orient='index', columns=[\"fold_0\", \"fold_1\", \"fold_2\", \"fold_3\", \"fold_4\"])\n",
    "table_dict = {\n",
    "              \"Mean Val Acc\": Q3_mean_val_acc,\n",
    "              \"Mean Val Loss\": Q3_mean_val_loss,\n",
    "              \"Mean Time Taken\": Q3_mean_time_taken,\n",
    "              \"Model Neurons list\": model_neurons_list,\n",
    "              \"Number of neurons\": num_neurons_list}\n",
    "data_df = pd.DataFrame.from_dict(table_dict)\n",
    "\n",
    "table_df.reset_index(drop=True, inplace=True)\n",
    "data_df.reset_index(drop=True, inplace=True)    \n",
    "\n",
    "table_df = pd.concat([table_df, data_df], axis=1)\n",
    "table_df.set_index('Number of neurons', inplace = True)\n",
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_no_neurons = int(table_df['Mean Val Acc'].idxmax())\n",
    "\n",
    "data = {\"Optimal Batch\": [optimal_batch_size], \"Optimal No.of neurons\": [optimal_no_neurons] }\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The selected optimal number of neurons in the hidden layer is 256. The rationale is that it yields the highest mean validation accuracy. It is possible that with the increase in number of neurons in the hidden layer, it allows the train model to be more adaptive and higher capacity to learn.**\n",
    "\n",
    "**In terms of time taken, when there is a increase number of neurons and the mean time taken for the final epoch does not change significantly nor any obvious relationship was observed thus time taken was not taken into consideration.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3C "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test accuracies against training epochs with the optimal number of neurons for different folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_1 = plt.figure(figsize=(15, 10))\n",
    "fold = 0\n",
    "optimal_neuron_model = str(table_df.loc[optimal_no_neurons, \"Model Neurons list\"])\n",
    "\n",
    "Q3_legend_list = []\n",
    "\n",
    "while(fold<no_folds):\n",
    "\n",
    "    plt.plot(Q3_history[optimal_neuron_model + model_fold[fold]].history[\"accuracy\"])\n",
    "    Q3_legend_list.append(\"Training Accuracy\" + model_fold[fold])\n",
    "\n",
    "    plt.plot(Q3_history[optimal_neuron_model + model_fold[fold]].history[\"val_accuracy\"])\n",
    "    Q3_legend_list.append(\"Validation Accuracy\" + model_fold[fold])\n",
    "    fold+=1\n",
    "\n",
    "plt.legend(Q3_legend_list)\n",
    "plt.title('K folds Accuracy vs Epochs for optimal number of neurons: ' + str(optimal_no_neurons))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3C_model = Sequential([Dense(optimal_no_neurons, activation='relu'),\n",
    "                Dropout(0.2), Dense(128, activation ='relu'),\n",
    "                Dropout(0.2), Dense(128, activation='relu'),\n",
    "                Dropout(0.2), Dense(1, activation='sigmoid')])\n",
    "\n",
    "Q3C_model.compile(optimizer='adam', \n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "Q3C_cb = TimingCallback()\n",
    "\n",
    "Q3C_history = {}\n",
    "Q3C_history[\"Q3C_model\"] = Q3C_model.fit(X_train_scaled, y_train,\n",
    "                batch_size = optimal_batch_size,\n",
    "                epochs=no_epochs,\n",
    "                verbose=1,\n",
    "                use_multiprocessing=True,\n",
    "                validation_data=(X_test_scaled, y_test), callbacks=[callback,  Q3C_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test accuracy against training epochs with the optimal number of neurons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_1 = plt.figure(figsize=(15, 10))\n",
    "plt.plot(Q3C_history['Q3C_model'].history['accuracy'])\n",
    "plt.plot(Q3C_history['Q3C_model'].history['val_accuracy'])\n",
    "plt.title('Accuracy vs Epochs for optimal no of neuron: ' + str(optimal_no_neurons))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3D \n",
    "#### How does dropout works, and what is the purpose of dropouts?\n",
    "\n",
    "**Dropout is a technique where randomly selected neurons are ignored during training to avoid overfitting. The neurons are presented with a probability p and presented to the next layer with weight W to the next layer at the training time. At test time, the weights are always present and presented to the network with weights multipled by probability p (dropout rate = 0.2). The output at the test time is same as the expected output at the training time**\n",
    "\n",
    "\n",
    "**Dropout prevents all neurons in a layer from syncrhonously optimizing their weights. It prevents all the neurons from  converging to the same goal, thus decorrelating the weights. As such, it removes the simple dependencies between the neurons and increases the robustness of the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3E \n",
    "#### Besides early stopping and dropout, what is another approach that you could take to address overfitting in the model, and how does it work? Implement the approach\n",
    "\n",
    "**Weight regularization. Weight regularization ensures that the weights of the network as large network weights indicates that the model is overfitting. The learning algorithm would encourage the network towards using small weights by penalizing the model with a larger loss score.**\n",
    "\n",
    "**Through experimenting with the values, L2 = 0.000001 yields a higher val accuracy score.**\n",
    "\n",
    "**I have used the model with the implementation of weight regularizations for the entirety of Question 4.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of weight regularization with optimal batch and number of neurons\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "optimized_model = Sequential([Dense(optimal_no_neurons, activation='relu', kernel_regularizer =regularizers.L2(0.000001)),\n",
    "                        Dropout(0.2), Dense(128, activation ='relu', kernel_regularizer =regularizers.L2(0.000001)),\n",
    "                        Dropout(0.2), Dense(128, activation='relu', kernel_regularizer =regularizers.L2(0.000001)),\n",
    "                        Dropout(0.2), Dense(1, activation='sigmoid')])\n",
    "\n",
    "# Default values for l1 and l2 are 0.01\n",
    "\n",
    "optimized_model.compile(optimizer='adam', \n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = {}\n",
    "history['optimized_model'] = optimized_model.fit(X_train_scaled, y_train,\n",
    "                            epochs = no_epochs, verbose = 1,\n",
    "                            batch_size = optimal_batch_size, validation_data = (X_test_scaled, y_test), callbacks = [callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.save('optimized_model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot of optimized model train test accuracies against epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_1 = plt.figure(figsize=(15, 10))\n",
    "plt.plot(history['optimized_model'].history['accuracy'])\n",
    "plt.plot(history['optimized_model'].history['val_accuracy'])\n",
    "plt.title('Optimized model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e20b04dd985343179d8656fb1930f451adca8610422f13b5e4759b9499c407bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
