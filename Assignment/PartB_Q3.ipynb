{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the seed here is sufficient. \n",
    "# If you don't plan to use these starter code, make sure you add this cell.\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import random \n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import pydot_ng as pydot\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Normalization, StringLookup, IntegerLookup\n",
    "from math import floor\n",
    "from math import sqrt \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>full_address</th>\n",
       "      <th>nearest_stn</th>\n",
       "      <th>dist_to_nearest_stn</th>\n",
       "      <th>dist_to_dhoby</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>eigenvector_centrality</th>\n",
       "      <th>flat_model_type</th>\n",
       "      <th>remaining_lease_years</th>\n",
       "      <th>floor_area_sqm</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>resale_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>406 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.007264</td>\n",
       "      <td>7.006044</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>2 ROOM, Improved</td>\n",
       "      <td>61.333333</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>232000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>108 ANG MO KIO AVENUE 4</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>1.271389</td>\n",
       "      <td>7.983837</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>60.583333</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>250000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>602 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.069743</td>\n",
       "      <td>9.090700</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.416667</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>262000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>465 ANG MO KIO AVENUE 10</td>\n",
       "      <td>Ang Mo Kio</td>\n",
       "      <td>0.946890</td>\n",
       "      <td>7.519889</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.083333</td>\n",
       "      <td>68.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>601 ANG MO KIO AVENUE 5</td>\n",
       "      <td>Yio Chu Kang</td>\n",
       "      <td>1.092551</td>\n",
       "      <td>9.130489</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>3 ROOM, New Generation</td>\n",
       "      <td>62.416667</td>\n",
       "      <td>67.0</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>265000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133407</th>\n",
       "      <td>6</td>\n",
       "      <td>2022</td>\n",
       "      <td>877 YISHUN STREET 81</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.475885</td>\n",
       "      <td>12.738721</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>EXECUTIVE, Maisonette</td>\n",
       "      <td>64.583333</td>\n",
       "      <td>145.0</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>810000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133408</th>\n",
       "      <td>1</td>\n",
       "      <td>2022</td>\n",
       "      <td>633 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.774113</td>\n",
       "      <td>13.229106</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>164.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>785000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133409</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>633 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.774113</td>\n",
       "      <td>13.229106</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.916667</td>\n",
       "      <td>171.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>842000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133410</th>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "      <td>632 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.700595</td>\n",
       "      <td>13.222912</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>164.0</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>845000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133411</th>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "      <td>605 YISHUN STREET 61</td>\n",
       "      <td>Khatib</td>\n",
       "      <td>0.603845</td>\n",
       "      <td>13.592586</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>MULTI-GENERATION, Multi Generation</td>\n",
       "      <td>64.750000</td>\n",
       "      <td>163.0</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>862000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133412 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        month  year              full_address   nearest_stn  \\\n",
       "0           1  2017  406 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "1           1  2017   108 ANG MO KIO AVENUE 4    Ang Mo Kio   \n",
       "2           1  2017   602 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "3           1  2017  465 ANG MO KIO AVENUE 10    Ang Mo Kio   \n",
       "4           1  2017   601 ANG MO KIO AVENUE 5  Yio Chu Kang   \n",
       "...       ...   ...                       ...           ...   \n",
       "133407      6  2022      877 YISHUN STREET 81        Khatib   \n",
       "133408      1  2022      633 YISHUN STREET 61        Khatib   \n",
       "133409      2  2022      633 YISHUN STREET 61        Khatib   \n",
       "133410      2  2022      632 YISHUN STREET 61        Khatib   \n",
       "133411      5  2022      605 YISHUN STREET 61        Khatib   \n",
       "\n",
       "        dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
       "0                  1.007264       7.006044           0.016807   \n",
       "1                  1.271389       7.983837           0.016807   \n",
       "2                  1.069743       9.090700           0.016807   \n",
       "3                  0.946890       7.519889           0.016807   \n",
       "4                  1.092551       9.130489           0.016807   \n",
       "...                     ...            ...                ...   \n",
       "133407             0.475885      12.738721           0.016807   \n",
       "133408             0.774113      13.229106           0.016807   \n",
       "133409             0.774113      13.229106           0.016807   \n",
       "133410             0.700595      13.222912           0.016807   \n",
       "133411             0.603845      13.592586           0.016807   \n",
       "\n",
       "        eigenvector_centrality                     flat_model_type  \\\n",
       "0                     0.006243                    2 ROOM, Improved   \n",
       "1                     0.006243              3 ROOM, New Generation   \n",
       "2                     0.002459              3 ROOM, New Generation   \n",
       "3                     0.006243              3 ROOM, New Generation   \n",
       "4                     0.002459              3 ROOM, New Generation   \n",
       "...                        ...                                 ...   \n",
       "133407                0.000968               EXECUTIVE, Maisonette   \n",
       "133408                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "133409                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "133410                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "133411                0.000968  MULTI-GENERATION, Multi Generation   \n",
       "\n",
       "        remaining_lease_years  floor_area_sqm storey_range  resale_price  \n",
       "0                   61.333333            44.0     10 TO 12      232000.0  \n",
       "1                   60.583333            67.0     01 TO 03      250000.0  \n",
       "2                   62.416667            67.0     01 TO 03      262000.0  \n",
       "3                   62.083333            68.0     04 TO 06      265000.0  \n",
       "4                   62.416667            67.0     01 TO 03      265000.0  \n",
       "...                       ...             ...          ...           ...  \n",
       "133407              64.583333           145.0     07 TO 09      810000.0  \n",
       "133408              65.000000           164.0     04 TO 06      785000.0  \n",
       "133409              64.916667           171.0     04 TO 06      842000.0  \n",
       "133410              64.750000           164.0     10 TO 12      845000.0  \n",
       "133411              64.750000           163.0     04 TO 06      862000.0  \n",
       "\n",
       "[133412 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions in this cell are adapted from https://keras.io/examples/structured_data/structured_data_classification_from_scratch/\n",
    "# It is the same link as the one mentioned in the question paper (Q1b)\n",
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    \n",
    "    labels = dataframe.pop(\"resale_price\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n",
    "\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode=\"binary\") # NOTE: as mentioned in the question paper, this actually does one-hot encoding. You could replace 'binary' with 'one_hot' if you wish to.\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def r2(y_true, y_pred): \n",
    "    '''\n",
    "    # Obtained from https://jmlb.github.io/ml/2017/03/20/CoeffDetermination_CustomMetric4Keras/\n",
    "    # TODO: you have to find out how to use it in your code\n",
    "    '''\n",
    "    SS_res = K.sum(K.square( y_true - y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "train_dataframe = df[df['year']<= 2020] \n",
    "test_dataframe =  df[df['year']>2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_not_used = [\"full_address\", \"nearest_stn\"]\n",
    "train_dataframe = train_dataframe.drop(category_not_used, axis = 1)\n",
    "test_dataframe = test_dataframe.drop(category_not_used, axis = 1)\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "test_ds = dataframe_to_dataset(test_dataframe)\n",
    "\n",
    "train_ds = train_ds.batch(256)\n",
    "test_ds = test_ds.batch(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical feature encoded as integer\n",
    "month = keras.Input(shape=(1,), name=\"month\", dtype=\"int64\")\n",
    "\n",
    "# Categorical feature encoded as string\n",
    "flat_model_type = keras.Input(shape=(1,), name=\"flat_model_type\", dtype=\"string\")\n",
    "storey_range = keras.Input(shape=(1,), name=\"storey_range\", dtype=\"string\")\n",
    "\n",
    "# Numerical features\n",
    "dist_to_nearest_stn = keras.Input(shape=(1,), name=\"dist_to_nearest_stn\")\n",
    "dist_to_dhoby = keras.Input(shape=(1,), name=\"dist_to_dhoby\")\n",
    "degree_centrality = keras.Input(shape=(1,), name=\"degree_centrality\")\n",
    "eigenvector_centrality = keras.Input(shape=(1,), name=\"eigenvector_centrality\")\n",
    "remaining_lease_years = keras.Input(shape=(1,), name=\"remaining_lease_years\")\n",
    "floor_area_sqm = keras.Input(shape=(1,), name=\"floor_area_sqm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = [month,flat_model_type,storey_range,dist_to_nearest_stn,\n",
    "            dist_to_dhoby,degree_centrality,eigenvector_centrality,remaining_lease_years,floor_area_sqm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q2_encode_categorical_feature(feature, name, dataset, is_string, num_categories, divisor):\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode=\"int\")\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x,-1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "\n",
    "    emb = layers.Embedding(input_dim=num_categories+1, output_dim=floor(num_categories//divisor))\n",
    "    embedded = emb(encoded_feature)\n",
    "\n",
    "    return layers.Flatten()(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_roots(l):\n",
    "    result = [sqrt(i) for i in l]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best hyperparamters value : 'learning_rate': 0.046185127256095915, 'divisor': 2, 'hidden_units': 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.046185127256095915\n",
    "divisor = 2\n",
    "hidden_units = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old test set\n",
    "Q3_old_df = pd.read_csv('hdb_price_prediction_old.csv')\n",
    "\n",
    "Q3_old_test_df = Q3_old_df[Q3_old_df[\"year\"]>2020].copy()\n",
    "Q3_old_test_df = Q3_old_test_df.drop(category_not_used, axis = 1)\n",
    "Q3_old_test_ds = dataframe_to_dataset(Q3_old_test_df)\n",
    "Q3_old_test_ds = Q3_old_test_ds.batch(128)\n",
    "\n",
    "# new test set\n",
    "Q3_new_test_2021_df = test_dataframe[test_dataframe[\"year\"]==2021].copy()\n",
    "Q3_new_test_2022_df = test_dataframe[test_dataframe[\"year\"]==2022].copy()\n",
    "\n",
    "Q3_new_test_2021_ds = dataframe_to_dataset(Q3_new_test_2021_df)\n",
    "Q3_new_test_2022_ds = dataframe_to_dataset(Q3_new_test_2022_df)\n",
    "\n",
    "Q3_new_test_2021_ds = Q3_new_test_2021_ds.batch(256)\n",
    "Q3_new_test_2022_ds = Q3_new_test_2022_ds.batch(256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using 2E model from the best epoch for 3A according to the clarification on the discussion board**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x19882902b00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q2E_model = keras.models.load_model('PartB_best_model/', custom_objects={\"r2\":r2})\n",
    "Q2E_model.load_weights('PartB_bestepoch/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old test set\n",
    "Q3_old_df = pd.read_csv('hdb_price_prediction_old.csv')\n",
    "\n",
    "Q3_old_test_df = Q3_old_df[Q3_old_df[\"year\"]>2020].copy()\n",
    "Q3_old_test_df = Q3_old_test_df.drop(category_not_used, axis = 1)\n",
    "Q3_old_test_ds = dataframe_to_dataset(Q3_old_test_df)\n",
    "Q3_old_test_ds = Q3_old_test_ds.batch(128)\n",
    "\n",
    "# new test set\n",
    "Q3_new_test_2021_df = test_dataframe[test_dataframe[\"year\"]==2021].copy()\n",
    "Q3_new_test_2022_df = test_dataframe[test_dataframe[\"year\"]==2022].copy()\n",
    "\n",
    "Q3_new_test_2021_ds = dataframe_to_dataset(Q3_new_test_2021_df)\n",
    "Q3_new_test_2022_ds = dataframe_to_dataset(Q3_new_test_2022_df)\n",
    "\n",
    "Q3_new_test_2021_ds = Q3_new_test_2021_ds.batch(256)\n",
    "Q3_new_test_2022_ds = Q3_new_test_2022_ds.batch(256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JoeTe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py:566: UserWarning: Input dict contained keys ['year'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 1s 2ms/step - loss: 5597425152.0000 - r2: 0.7832\n",
      "Old Test RMSE: 74815.941831\n",
      "Old Test R^2: 0.783178\n",
      "114/114 [==============================] - 0s 2ms/step - loss: 6625486336.0000 - r2: 0.7472\n",
      "Test RMSE_2021: 81397.090464\n",
      "Test R^2_2021: 0.747247\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 14154502144.0000 - r2: 0.5040\n",
      "Test RMSE_2022: 118972.694951\n",
      "Test R^2_2022: 0.504003\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Old Test RMSE</th>\n",
       "      <th>Old Test R^2</th>\n",
       "      <th>Test RMSE_2021</th>\n",
       "      <th>Test R^2_2021</th>\n",
       "      <th>Test RMSE_2022</th>\n",
       "      <th>Test R^2_2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74815.941831</td>\n",
       "      <td>0.783178</td>\n",
       "      <td>81397.090464</td>\n",
       "      <td>0.747247</td>\n",
       "      <td>1.415450e+10</td>\n",
       "      <td>0.504003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Old Test RMSE  Old Test R^2  Test RMSE_2021  Test R^2_2021  Test RMSE_2022  \\\n",
       "0   74815.941831      0.783178    81397.090464       0.747247    1.415450e+10   \n",
       "\n",
       "   Test R^2_2022  \n",
       "0       0.504003  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_test_loss, old_test_R2 = Q2E_model.evaluate(Q3_old_test_ds)\n",
    "print('Old Test RMSE: %f' % sqrt(old_test_loss))\n",
    "print('Old Test R^2: %f' % old_test_R2)\n",
    "\n",
    "new_test_2021_loss, new_test_2021_R2 = Q2E_model.evaluate(Q3_new_test_2021_ds)\n",
    "print('Test RMSE_2021: %f' % sqrt(new_test_2021_loss))\n",
    "print('Test R^2_2021: %f' % (new_test_2021_R2))\n",
    "\n",
    "new_test_2022_loss, new_test_2022_R2 = Q2E_model.evaluate(Q3_new_test_2022_ds)\n",
    "print('Test RMSE_2022: %f' % sqrt(new_test_2022_loss))\n",
    "print('Test R^2_2022: %f' % (new_test_2022_R2))\n",
    "\n",
    "#Output in a table\n",
    "\n",
    "data = {\"Old Test RMSE\": [sqrt(old_test_loss)],\n",
    "        \"Old Test R^2\": [(old_test_R2)],\n",
    "        \"Test RMSE_2021\": [sqrt(new_test_2021_loss)],\n",
    "        \"Test R^2_2021\": [(new_test_2021_R2)],\n",
    "        \"Test RMSE_2022\": [(new_test_2022_loss)],\n",
    "        \"Test R^2_2022\": [(new_test_2022_R2)]}\n",
    "\n",
    "data_df = pd.DataFrame.from_dict(data)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3B \n",
    "#### Compare the extent to which model degradation has impacted your model to that of the team's linear regression model and explain why this has occured.\n",
    "\n",
    "| Linear Regression Model |  R^2 value |            \n",
    "| --- | --- |\n",
    "| Q1B Test > 2020 | .627 |\n",
    "| Q3B Old Test > 2020  | .760 |\n",
    "| Q3B Test  == 2021  | .715 |\n",
    "| Q3B Test  == 2022  | .464 |\n",
    "\n",
    "--**Neural Network Model**--\n",
    "\n",
    "| NN model |  R^2 value |\n",
    "| --- | --- |\n",
    "| Q3B Old Test > 2020  | .783178 |\n",
    "| Q3B Test  == 2021  | .747247 |\n",
    "| Q3B Test  == 2022  | .504003 |\n",
    "\n",
    "\n",
    "**Model degradation has led to a more signiifcant drop in R^2 value in the linear regression model as compared to the neural network model. As the multi-layer neural network has the ability to capture non-linear relationships, even though there is a change in characteristics of the dataset, the neural network model still performs better.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying Covariate shift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we used train_ds for best_model, we are going to use train_dataframe\n",
    "\n",
    "train_df_shift = train_dataframe.copy()\n",
    "Q3_old_test_df_shift = Q3_old_test_df.copy()\n",
    "test_df_2021_shift = Q3_new_test_2021_df.copy()\n",
    "test_df_2022_shift = Q3_new_test_2022_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1440 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes =plt.subplots(1,3, figsize=(20,20))\n",
    "axes[0].hist([train_df_shift[\"month\"], test_df_2021_shift[\"month\"],test_df_2022_shift[\"month\"]],bins = 12, histtype='bar', color =['red', 'green', 'blue'], label=['train_df_month', 'test_df_2021_month', 'test_df_2022_month'],rwidth=0.8)\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].set_title('Month Histogram')\n",
    "axes[0].set_xticks(range(1,13))\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist([train_df_shift[\"degree_centrality\"], test_df_2021_shift[\"degree_centrality\"],test_df_2022_shift[\"degree_centrality\"]],bins = np.linspace(0, 0.05, num=10), histtype='bar', color =['red', 'green', 'blue'], label=['train_df_centrality', 'test_df_2021_centrality', 'test_df_2022_centrality'],rwidth=0.8)\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].set_title('Degree Centrality')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "axes[2].hist([train_df_shift[\"eigenvector_centrality\"], test_df_2021_shift[\"eigenvector_centrality\"],test_df_2022_shift[\"eigenvector_centrality\"]],bins = np.linspace(0, 0.01, num=10), histtype='bar', color =['red', 'green', 'blue'], label=['train_df_eigenvector', 'test_df_2021_eigenvector', 'test_df_2022_eigenvector'],rwidth=0.8)\n",
    "axes[2].legend(loc='upper right')\n",
    "axes[2].set_title('Eigenvector Centrality')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes =plt.subplots(1, figsize=(20,20))\n",
    "axes.hist([train_df_shift[\"storey_range\"], test_df_2021_shift[\"storey_range\"],test_df_2022_shift[\"storey_range\"]],bins = train_df_shift[\"storey_range\"].nunique(), histtype='bar', color =['red', 'green', 'blue'], label=['train_df_storey_range', 'test_df_2021_storey_range', 'test_df_2022_storey_range'],rwidth=0.8, orientation='horizontal')\n",
    "axes.legend(loc='upper right')\n",
    "axes.set_title('Storey range Histogram')\n",
    "axes.set_xlabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3C \n",
    "\n",
    "#### Which variable showed the largest covariate/label shift that might have led to the drop in model performance as seen in 3b? \n",
    "\n",
    "**From the histogram plot, the Month has the largest distribution shift within the train and test dataset which could be the reason for the drop in R^2 values**\n",
    "\n",
    "**The easiest way would be to drop the features which are being classifed as drifiting, however this might result in a loss of information**. **Alternatively, it would be to retrieve more updated dataset and integrate into training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3D & 3E\n",
    "\n",
    "#### RFE.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "ix 0 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JoeTe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py:566: UserWarning: Input dict contained keys ['year'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342/342 [==============================] - 2s 4ms/step - loss: 84233969664.0000 - r2: -2.5428 - val_loss: 17914851328.0000 - val_r2: 0.3487\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9193890816.0000 - r2: 0.6115 - val_loss: 15100820480.0000 - val_r2: 0.4486\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7906354176.0000 - r2: 0.6649 - val_loss: 14291372032.0000 - val_r2: 0.4771\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7305051648.0000 - r2: 0.6904 - val_loss: 13262029824.0000 - val_r2: 0.5147\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6865902080.0000 - r2: 0.7088 - val_loss: 12781097984.0000 - val_r2: 0.5317\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6460293120.0000 - r2: 0.7258 - val_loss: 12580141056.0000 - val_r2: 0.5400\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6078106112.0000 - r2: 0.7424 - val_loss: 12306676736.0000 - val_r2: 0.5500\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5718280704.0000 - r2: 0.7573 - val_loss: 11234048000.0000 - val_r2: 0.5888\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5402378240.0000 - r2: 0.7711 - val_loss: 11167117312.0000 - val_r2: 0.5907\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5145071616.0000 - r2: 0.7814 - val_loss: 10686197760.0000 - val_r2: 0.6088\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4920711680.0000 - r2: 0.7915 - val_loss: 10606539776.0000 - val_r2: 0.6117\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4730554880.0000 - r2: 0.7989 - val_loss: 10828210176.0000 - val_r2: 0.6032\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4558792704.0000 - r2: 0.8062 - val_loss: 11062680576.0000 - val_r2: 0.5951\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4407778304.0000 - r2: 0.8131 - val_loss: 11093219328.0000 - val_r2: 0.5933\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4271872768.0000 - r2: 0.8186 - val_loss: 10455182336.0000 - val_r2: 0.6170\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4156879616.0000 - r2: 0.8235 - val_loss: 10005659648.0000 - val_r2: 0.6335\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4060937728.0000 - r2: 0.8275 - val_loss: 9786472448.0000 - val_r2: 0.6410\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3986707712.0000 - r2: 0.8307 - val_loss: 9877778432.0000 - val_r2: 0.6387\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3925940736.0000 - r2: 0.8330 - val_loss: 10563970048.0000 - val_r2: 0.6129\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3880566784.0000 - r2: 0.8351 - val_loss: 10277425152.0000 - val_r2: 0.6228\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3836108032.0000 - r2: 0.8374 - val_loss: 10462694400.0000 - val_r2: 0.6163\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3812864512.0000 - r2: 0.8383 - val_loss: 9357457408.0000 - val_r2: 0.6572\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3787199232.0000 - r2: 0.8392 - val_loss: 10147275776.0000 - val_r2: 0.6284\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3777229312.0000 - r2: 0.8393 - val_loss: 10972878848.0000 - val_r2: 0.5981\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3758003456.0000 - r2: 0.8401 - val_loss: 10113007616.0000 - val_r2: 0.6295\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3748783616.0000 - r2: 0.8407 - val_loss: 10314356736.0000 - val_r2: 0.6226\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3736385792.0000 - r2: 0.8411 - val_loss: 10037955584.0000 - val_r2: 0.6318\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3729478912.0000 - r2: 0.8416 - val_loss: 10081419264.0000 - val_r2: 0.6304\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3723907584.0000 - r2: 0.8417 - val_loss: 10166334464.0000 - val_r2: 0.6276\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3714027776.0000 - r2: 0.8421 - val_loss: 10441419776.0000 - val_r2: 0.6170\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3712171008.0000 - r2: 0.8422 - val_loss: 10423503872.0000 - val_r2: 0.6182\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3703762432.0000 - r2: 0.8425 - val_loss: 10045666304.0000 - val_r2: 0.6320\n",
      "new min loss: len 9, ix 0\n",
      "session cleared!\n",
      "\n",
      "ix 1 i 1\n",
      "updated temp_vec [1, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 3s 6ms/step - loss: 85261770752.0000 - r2: -2.6320 - val_loss: 19633022976.0000 - val_r2: 0.2867\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 5ms/step - loss: 10591634432.0000 - r2: 0.5526 - val_loss: 17218633728.0000 - val_r2: 0.3718\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9515292672.0000 - r2: 0.5981 - val_loss: 16059164672.0000 - val_r2: 0.4143\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8894954496.0000 - r2: 0.6235 - val_loss: 16265158656.0000 - val_r2: 0.4066\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8360264192.0000 - r2: 0.6459 - val_loss: 15291811840.0000 - val_r2: 0.4413\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7840298496.0000 - r2: 0.6683 - val_loss: 14026286080.0000 - val_r2: 0.4883\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7327800320.0000 - r2: 0.6891 - val_loss: 13854793728.0000 - val_r2: 0.4937\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6847192064.0000 - r2: 0.7101 - val_loss: 13228857344.0000 - val_r2: 0.5170\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6420887040.0000 - r2: 0.7286 - val_loss: 12891869184.0000 - val_r2: 0.5291\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6074100736.0000 - r2: 0.7429 - val_loss: 13015591936.0000 - val_r2: 0.5240\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5784262144.0000 - r2: 0.7550 - val_loss: 13211977728.0000 - val_r2: 0.5170\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5541690368.0000 - r2: 0.7652 - val_loss: 12599604224.0000 - val_r2: 0.5399\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5331721216.0000 - r2: 0.7741 - val_loss: 12559361024.0000 - val_r2: 0.5400\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5146573312.0000 - r2: 0.7817 - val_loss: 12059112448.0000 - val_r2: 0.5588\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4984705536.0000 - r2: 0.7884 - val_loss: 12525056000.0000 - val_r2: 0.5422\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4856171008.0000 - r2: 0.7937 - val_loss: 11924646912.0000 - val_r2: 0.5629\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4743585280.0000 - r2: 0.7991 - val_loss: 11237491712.0000 - val_r2: 0.5886\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4660304896.0000 - r2: 0.8025 - val_loss: 10346027008.0000 - val_r2: 0.6215\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4588898304.0000 - r2: 0.8056 - val_loss: 11660153856.0000 - val_r2: 0.5722\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4534401024.0000 - r2: 0.8079 - val_loss: 11552970752.0000 - val_r2: 0.5767\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4497468928.0000 - r2: 0.8089 - val_loss: 11117726720.0000 - val_r2: 0.5938\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4458612736.0000 - r2: 0.8107 - val_loss: 11480393728.0000 - val_r2: 0.5793\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4441649152.0000 - r2: 0.8118 - val_loss: 10385385472.0000 - val_r2: 0.6200\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4420738048.0000 - r2: 0.8122 - val_loss: 10623262720.0000 - val_r2: 0.6109\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4403219968.0000 - r2: 0.8133 - val_loss: 11046177792.0000 - val_r2: 0.5966\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4393296384.0000 - r2: 0.8134 - val_loss: 11356088320.0000 - val_r2: 0.5844\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4385645568.0000 - r2: 0.8141 - val_loss: 11382678528.0000 - val_r2: 0.5828\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4378764800.0000 - r2: 0.8143 - val_loss: 11153591296.0000 - val_r2: 0.5919\n",
      "session cleared!\n",
      "\n",
      "ix 2 i 1\n",
      "updated temp_vec [1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 102494937088.0000 - r2: -3.3473 - val_loss: 30999971840.0000 - val_r2: -0.1290\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 19509213184.0000 - r2: 0.1767 - val_loss: 27977291776.0000 - val_r2: -0.0200\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 17737322496.0000 - r2: 0.2496 - val_loss: 25948788736.0000 - val_r2: 0.0535\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 16531158016.0000 - r2: 0.3001 - val_loss: 24883771392.0000 - val_r2: 0.0904\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 15369210880.0000 - r2: 0.3493 - val_loss: 23583514624.0000 - val_r2: 0.1376\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 14087512064.0000 - r2: 0.4034 - val_loss: 22505607168.0000 - val_r2: 0.1770\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 12650244096.0000 - r2: 0.4642 - val_loss: 19992442880.0000 - val_r2: 0.2696\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 11077565440.0000 - r2: 0.5312 - val_loss: 19677661184.0000 - val_r2: 0.2805\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9459326976.0000 - r2: 0.5997 - val_loss: 16731126784.0000 - val_r2: 0.3877\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7930593792.0000 - r2: 0.6641 - val_loss: 14500708352.0000 - val_r2: 0.4692\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6644866048.0000 - r2: 0.7186 - val_loss: 14084695040.0000 - val_r2: 0.4852\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 5682263040.0000 - r2: 0.7590 - val_loss: 13421562880.0000 - val_r2: 0.5088\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5034625536.0000 - r2: 0.7866 - val_loss: 11725064192.0000 - val_r2: 0.5707\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4647222272.0000 - r2: 0.8026 - val_loss: 11764769792.0000 - val_r2: 0.5694\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4444011520.0000 - r2: 0.8114 - val_loss: 11378088960.0000 - val_r2: 0.5827\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4345747968.0000 - r2: 0.8154 - val_loss: 10819470336.0000 - val_r2: 0.6036\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4315298304.0000 - r2: 0.8169 - val_loss: 11498280960.0000 - val_r2: 0.5787\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4300529152.0000 - r2: 0.8172 - val_loss: 10470012928.0000 - val_r2: 0.6168\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4301428224.0000 - r2: 0.8175 - val_loss: 11587800064.0000 - val_r2: 0.5762\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4296964608.0000 - r2: 0.8175 - val_loss: 11552980992.0000 - val_r2: 0.5768\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4295818240.0000 - r2: 0.8177 - val_loss: 10809815040.0000 - val_r2: 0.6040\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4294925312.0000 - r2: 0.8178 - val_loss: 10960581632.0000 - val_r2: 0.5986\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4294653184.0000 - r2: 0.8173 - val_loss: 10847040512.0000 - val_r2: 0.6023\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4297402880.0000 - r2: 0.8174 - val_loss: 10962416640.0000 - val_r2: 0.5987\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4296814592.0000 - r2: 0.8172 - val_loss: 10995421184.0000 - val_r2: 0.5968\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4294394624.0000 - r2: 0.8175 - val_loss: 10424577024.0000 - val_r2: 0.6178\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4294155008.0000 - r2: 0.8177 - val_loss: 11505511424.0000 - val_r2: 0.5791\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4292388352.0000 - r2: 0.8178 - val_loss: 10658909184.0000 - val_r2: 0.6097\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4296458752.0000 - r2: 0.8175 - val_loss: 11015828480.0000 - val_r2: 0.5968\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4295876096.0000 - r2: 0.8176 - val_loss: 11409266688.0000 - val_r2: 0.5813\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4298457600.0000 - r2: 0.8176 - val_loss: 10404177920.0000 - val_r2: 0.6192\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4298015744.0000 - r2: 0.8175 - val_loss: 10348520448.0000 - val_r2: 0.6215\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4295517696.0000 - r2: 0.8174 - val_loss: 11412959232.0000 - val_r2: 0.5819\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4297567232.0000 - r2: 0.8180 - val_loss: 11189014528.0000 - val_r2: 0.5895\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4293269760.0000 - r2: 0.8177 - val_loss: 11925702656.0000 - val_r2: 0.5633\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4296138752.0000 - r2: 0.8175 - val_loss: 11661296640.0000 - val_r2: 0.5733\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4295155712.0000 - r2: 0.8177 - val_loss: 11001415680.0000 - val_r2: 0.5973\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4291897600.0000 - r2: 0.8176 - val_loss: 11315067904.0000 - val_r2: 0.5858\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4296313344.0000 - r2: 0.8176 - val_loss: 11400420352.0000 - val_r2: 0.5821\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4297419264.0000 - r2: 0.8176 - val_loss: 11366893568.0000 - val_r2: 0.5836\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4295795712.0000 - r2: 0.8175 - val_loss: 11170376704.0000 - val_r2: 0.5905\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4298819072.0000 - r2: 0.8178 - val_loss: 11259379712.0000 - val_r2: 0.5872\n",
      "session cleared!\n",
      "\n",
      "ix 3 i 1\n",
      "updated temp_vec [1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 80495673344.0000 - r2: -2.4372 - val_loss: 17870663680.0000 - val_r2: 0.3501\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9202412544.0000 - r2: 0.6111 - val_loss: 15069891584.0000 - val_r2: 0.4502\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7963362816.0000 - r2: 0.6625 - val_loss: 14381225984.0000 - val_r2: 0.4746\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7388098560.0000 - r2: 0.6870 - val_loss: 13286060032.0000 - val_r2: 0.5141\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6963076608.0000 - r2: 0.7045 - val_loss: 13144882176.0000 - val_r2: 0.5195\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6563250688.0000 - r2: 0.7215 - val_loss: 12646665216.0000 - val_r2: 0.5368\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6179361280.0000 - r2: 0.7382 - val_loss: 12423191552.0000 - val_r2: 0.5458\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5814233600.0000 - r2: 0.7539 - val_loss: 11836732416.0000 - val_r2: 0.5669\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5498119168.0000 - r2: 0.7667 - val_loss: 11320582144.0000 - val_r2: 0.5858\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5232248320.0000 - r2: 0.7779 - val_loss: 11286522880.0000 - val_r2: 0.5872\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5018023936.0000 - r2: 0.7869 - val_loss: 11062874112.0000 - val_r2: 0.5940\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4835913216.0000 - r2: 0.7946 - val_loss: 10545971200.0000 - val_r2: 0.6140\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4686564864.0000 - r2: 0.8014 - val_loss: 10428934144.0000 - val_r2: 0.6177\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4550263296.0000 - r2: 0.8066 - val_loss: 10919398400.0000 - val_r2: 0.5998\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4437322752.0000 - r2: 0.8114 - val_loss: 11104902144.0000 - val_r2: 0.5940\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4336629248.0000 - r2: 0.8158 - val_loss: 11001410560.0000 - val_r2: 0.5972\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4264702720.0000 - r2: 0.8186 - val_loss: 10870009856.0000 - val_r2: 0.6016\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4200787200.0000 - r2: 0.8215 - val_loss: 10283597824.0000 - val_r2: 0.6233\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4147219200.0000 - r2: 0.8238 - val_loss: 9934967808.0000 - val_r2: 0.6359\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4117580544.0000 - r2: 0.8248 - val_loss: 10374819840.0000 - val_r2: 0.6199\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4086652160.0000 - r2: 0.8262 - val_loss: 11558983680.0000 - val_r2: 0.5767\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4066028032.0000 - r2: 0.8273 - val_loss: 10714385408.0000 - val_r2: 0.6076\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4049172736.0000 - r2: 0.8280 - val_loss: 10346795008.0000 - val_r2: 0.6208\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4045679360.0000 - r2: 0.8280 - val_loss: 10423843840.0000 - val_r2: 0.6178\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4037679616.0000 - r2: 0.8282 - val_loss: 9867473920.0000 - val_r2: 0.6386\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4034238208.0000 - r2: 0.8285 - val_loss: 9957960704.0000 - val_r2: 0.6347\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4028502016.0000 - r2: 0.8283 - val_loss: 10889845760.0000 - val_r2: 0.6005\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4030767616.0000 - r2: 0.8287 - val_loss: 9982628864.0000 - val_r2: 0.6341\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4034046464.0000 - r2: 0.8284 - val_loss: 10357706752.0000 - val_r2: 0.6205\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4029967872.0000 - r2: 0.8289 - val_loss: 9798505472.0000 - val_r2: 0.6400\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4026867968.0000 - r2: 0.8288 - val_loss: 9882098688.0000 - val_r2: 0.6374\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4025907712.0000 - r2: 0.8291 - val_loss: 9466027008.0000 - val_r2: 0.6532\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4026418176.0000 - r2: 0.8289 - val_loss: 10748668928.0000 - val_r2: 0.6051\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4029605376.0000 - r2: 0.8289 - val_loss: 9761126400.0000 - val_r2: 0.6421\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4031402752.0000 - r2: 0.8288 - val_loss: 10250723328.0000 - val_r2: 0.6240\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4031625984.0000 - r2: 0.8288 - val_loss: 10609468416.0000 - val_r2: 0.6100\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4028155136.0000 - r2: 0.8288 - val_loss: 10410066944.0000 - val_r2: 0.6185\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4025265152.0000 - r2: 0.8287 - val_loss: 10252887040.0000 - val_r2: 0.6237\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4025591040.0000 - r2: 0.8290 - val_loss: 10318706688.0000 - val_r2: 0.6209\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4033463552.0000 - r2: 0.8282 - val_loss: 10539613184.0000 - val_r2: 0.6138\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4026807552.0000 - r2: 0.8290 - val_loss: 10363836416.0000 - val_r2: 0.6203\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4025837056.0000 - r2: 0.8290 - val_loss: 11176003584.0000 - val_r2: 0.5898\n",
      "session cleared!\n",
      "\n",
      "ix 4 i 1\n",
      "updated temp_vec [1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 80711655424.0000 - r2: -2.4033 - val_loss: 18232870912.0000 - val_r2: 0.3377\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9234685952.0000 - r2: 0.6101 - val_loss: 15131847680.0000 - val_r2: 0.4470\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7977432064.0000 - r2: 0.6619 - val_loss: 13883782144.0000 - val_r2: 0.4932\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7412662272.0000 - r2: 0.6846 - val_loss: 13950188544.0000 - val_r2: 0.4901\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6997133312.0000 - r2: 0.7035 - val_loss: 12539573248.0000 - val_r2: 0.5416\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6622203392.0000 - r2: 0.7195 - val_loss: 12998205440.0000 - val_r2: 0.5242\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6267579392.0000 - r2: 0.7337 - val_loss: 12529460224.0000 - val_r2: 0.5414\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 5ms/step - loss: 5945974272.0000 - r2: 0.7477 - val_loss: 12429953024.0000 - val_r2: 0.5447\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5664828416.0000 - r2: 0.7599 - val_loss: 12277330944.0000 - val_r2: 0.5504\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5441388544.0000 - r2: 0.7688 - val_loss: 11942919168.0000 - val_r2: 0.5624\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5261246976.0000 - r2: 0.7767 - val_loss: 11294402560.0000 - val_r2: 0.5867\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5110703104.0000 - r2: 0.7828 - val_loss: 11565496320.0000 - val_r2: 0.5763\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4979515392.0000 - r2: 0.7887 - val_loss: 11568473088.0000 - val_r2: 0.5762\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4864920064.0000 - r2: 0.7936 - val_loss: 11210907648.0000 - val_r2: 0.5891\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4755517952.0000 - r2: 0.7979 - val_loss: 10911067136.0000 - val_r2: 0.5998\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4661481984.0000 - r2: 0.8020 - val_loss: 11462226944.0000 - val_r2: 0.5802\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4583750656.0000 - r2: 0.8055 - val_loss: 10438235136.0000 - val_r2: 0.6174\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4522015232.0000 - r2: 0.8078 - val_loss: 10574886912.0000 - val_r2: 0.6128\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4469915136.0000 - r2: 0.8098 - val_loss: 11054543872.0000 - val_r2: 0.5952\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4434426368.0000 - r2: 0.8115 - val_loss: 10639208448.0000 - val_r2: 0.6094\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4408077312.0000 - r2: 0.8127 - val_loss: 11063087104.0000 - val_r2: 0.5946\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4393796608.0000 - r2: 0.8135 - val_loss: 10329194496.0000 - val_r2: 0.6217\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4375490560.0000 - r2: 0.8143 - val_loss: 10568276992.0000 - val_r2: 0.6130\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4365385728.0000 - r2: 0.8147 - val_loss: 11027198976.0000 - val_r2: 0.5960\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4355033600.0000 - r2: 0.8147 - val_loss: 10241219584.0000 - val_r2: 0.6246\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4350126592.0000 - r2: 0.8154 - val_loss: 10446946304.0000 - val_r2: 0.6175\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4354115072.0000 - r2: 0.8151 - val_loss: 11587213312.0000 - val_r2: 0.5749\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4352003584.0000 - r2: 0.8149 - val_loss: 11166474240.0000 - val_r2: 0.5909\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4347476992.0000 - r2: 0.8154 - val_loss: 11686495232.0000 - val_r2: 0.5723\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4351278080.0000 - r2: 0.8152 - val_loss: 11057091584.0000 - val_r2: 0.5950\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4349202944.0000 - r2: 0.8150 - val_loss: 10264541184.0000 - val_r2: 0.6235\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4347315200.0000 - r2: 0.8154 - val_loss: 11440153600.0000 - val_r2: 0.5808\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4345354752.0000 - r2: 0.8153 - val_loss: 10376419328.0000 - val_r2: 0.6187\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4350939136.0000 - r2: 0.8152 - val_loss: 10426923008.0000 - val_r2: 0.6171\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4349132288.0000 - r2: 0.8151 - val_loss: 10596150272.0000 - val_r2: 0.6124\n",
      "session cleared!\n",
      "\n",
      "ix 5 i 1\n",
      "updated temp_vec [1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "going through feature_mask [1, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 80398843904.0000 - r2: -2.3915 - val_loss: 17927591936.0000 - val_r2: 0.3481\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9302264832.0000 - r2: 0.6064 - val_loss: 14980526080.0000 - val_r2: 0.4540\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8118947328.0000 - r2: 0.6559 - val_loss: 14407076864.0000 - val_r2: 0.4740\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7576016384.0000 - r2: 0.6784 - val_loss: 13424757760.0000 - val_r2: 0.5094\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7180418560.0000 - r2: 0.6951 - val_loss: 13202658304.0000 - val_r2: 0.5179\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6808173056.0000 - r2: 0.7114 - val_loss: 12264760320.0000 - val_r2: 0.5519\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6437244928.0000 - r2: 0.7271 - val_loss: 12130525184.0000 - val_r2: 0.5567\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6059947008.0000 - r2: 0.7427 - val_loss: 12054940672.0000 - val_r2: 0.5591\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5704654336.0000 - r2: 0.7582 - val_loss: 12802484224.0000 - val_r2: 0.5313\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5395784704.0000 - r2: 0.7713 - val_loss: 11700237312.0000 - val_r2: 0.5721\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5126403072.0000 - r2: 0.7829 - val_loss: 11407868928.0000 - val_r2: 0.5828\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4907173376.0000 - r2: 0.7916 - val_loss: 11656908800.0000 - val_r2: 0.5726\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4728506368.0000 - r2: 0.7990 - val_loss: 10740834304.0000 - val_r2: 0.6066\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4566230528.0000 - r2: 0.8063 - val_loss: 11687936000.0000 - val_r2: 0.5721\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4422736896.0000 - r2: 0.8121 - val_loss: 11053454336.0000 - val_r2: 0.5952\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4302544384.0000 - r2: 0.8173 - val_loss: 10791914496.0000 - val_r2: 0.6047\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4205160448.0000 - r2: 0.8212 - val_loss: 10386314240.0000 - val_r2: 0.6197\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4119240704.0000 - r2: 0.8253 - val_loss: 10105574400.0000 - val_r2: 0.6299\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4053384704.0000 - r2: 0.8278 - val_loss: 10690438144.0000 - val_r2: 0.6080\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4001106688.0000 - r2: 0.8298 - val_loss: 9948614656.0000 - val_r2: 0.6352\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3962021120.0000 - r2: 0.8315 - val_loss: 10594677760.0000 - val_r2: 0.6110\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3933258752.0000 - r2: 0.8328 - val_loss: 10798641152.0000 - val_r2: 0.6041\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3910182400.0000 - r2: 0.8339 - val_loss: 10988827648.0000 - val_r2: 0.5967\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3885136896.0000 - r2: 0.8352 - val_loss: 11367307264.0000 - val_r2: 0.5836\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3876626432.0000 - r2: 0.8351 - val_loss: 10585232384.0000 - val_r2: 0.6109\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3864109056.0000 - r2: 0.8355 - val_loss: 10306058240.0000 - val_r2: 0.6221\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3849888256.0000 - r2: 0.8363 - val_loss: 10761812992.0000 - val_r2: 0.6046\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3846124544.0000 - r2: 0.8365 - val_loss: 9740153856.0000 - val_r2: 0.6426\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3833835520.0000 - r2: 0.8372 - val_loss: 10296077312.0000 - val_r2: 0.6228\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3827265792.0000 - r2: 0.8373 - val_loss: 9998372864.0000 - val_r2: 0.6327\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3822737664.0000 - r2: 0.8373 - val_loss: 10598210560.0000 - val_r2: 0.6111\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3812735232.0000 - r2: 0.8382 - val_loss: 10312808448.0000 - val_r2: 0.6221\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3809062400.0000 - r2: 0.8383 - val_loss: 9991796736.0000 - val_r2: 0.6339\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3804163584.0000 - r2: 0.8383 - val_loss: 10487187456.0000 - val_r2: 0.6155\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3802187520.0000 - r2: 0.8385 - val_loss: 9794363392.0000 - val_r2: 0.6412\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3806201600.0000 - r2: 0.8381 - val_loss: 11287945216.0000 - val_r2: 0.5862\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3797568256.0000 - r2: 0.8387 - val_loss: 10275117056.0000 - val_r2: 0.6231\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3790469888.0000 - r2: 0.8389 - val_loss: 10649269248.0000 - val_r2: 0.6096\n",
      "session cleared!\n",
      "\n",
      "ix 6 i 1\n",
      "updated temp_vec [1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "going through feature_mask [1, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 80080289792.0000 - r2: -2.4051 - val_loss: 17803132928.0000 - val_r2: 0.3526\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9334406144.0000 - r2: 0.6051 - val_loss: 14682474496.0000 - val_r2: 0.4639\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8150084608.0000 - r2: 0.6547 - val_loss: 14057127936.0000 - val_r2: 0.4857\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7612501504.0000 - r2: 0.6776 - val_loss: 13795707904.0000 - val_r2: 0.4950\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7222210560.0000 - r2: 0.6940 - val_loss: 12871858176.0000 - val_r2: 0.5287\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6849555968.0000 - r2: 0.7094 - val_loss: 12853231616.0000 - val_r2: 0.5298\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6470360576.0000 - r2: 0.7253 - val_loss: 12347099136.0000 - val_r2: 0.5486\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6069558784.0000 - r2: 0.7427 - val_loss: 12301194240.0000 - val_r2: 0.5496\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5676581888.0000 - r2: 0.7592 - val_loss: 11909481472.0000 - val_r2: 0.5641\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5305610752.0000 - r2: 0.7747 - val_loss: 11855142912.0000 - val_r2: 0.5658\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4977228800.0000 - r2: 0.7890 - val_loss: 11507351552.0000 - val_r2: 0.5783\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4692942848.0000 - r2: 0.8008 - val_loss: 11045811200.0000 - val_r2: 0.5962\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4451047936.0000 - r2: 0.8108 - val_loss: 10722797568.0000 - val_r2: 0.6071\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4255181568.0000 - r2: 0.8191 - val_loss: 10986463232.0000 - val_r2: 0.5972\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4096100608.0000 - r2: 0.8265 - val_loss: 9943513088.0000 - val_r2: 0.6348\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3991401984.0000 - r2: 0.8304 - val_loss: 10077490176.0000 - val_r2: 0.6306\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3914690304.0000 - r2: 0.8336 - val_loss: 10446841856.0000 - val_r2: 0.6171\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3865083904.0000 - r2: 0.8358 - val_loss: 10470786048.0000 - val_r2: 0.6162\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3835939328.0000 - r2: 0.8368 - val_loss: 11169783808.0000 - val_r2: 0.5901\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3816835584.0000 - r2: 0.8378 - val_loss: 11137601536.0000 - val_r2: 0.5914\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3802668800.0000 - r2: 0.8384 - val_loss: 10131096576.0000 - val_r2: 0.6289\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3784996352.0000 - r2: 0.8393 - val_loss: 10241388544.0000 - val_r2: 0.6242\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3780587776.0000 - r2: 0.8392 - val_loss: 9483613184.0000 - val_r2: 0.6521\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3767889920.0000 - r2: 0.8401 - val_loss: 9986979840.0000 - val_r2: 0.6336\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3756922624.0000 - r2: 0.8401 - val_loss: 9501573120.0000 - val_r2: 0.6516\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3751091200.0000 - r2: 0.8407 - val_loss: 10717040640.0000 - val_r2: 0.6077\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3746099712.0000 - r2: 0.8408 - val_loss: 10358139904.0000 - val_r2: 0.6197\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3732866560.0000 - r2: 0.8410 - val_loss: 9890068480.0000 - val_r2: 0.6374\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3724982784.0000 - r2: 0.8417 - val_loss: 10497672192.0000 - val_r2: 0.6151\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3718452736.0000 - r2: 0.8419 - val_loss: 9602296832.0000 - val_r2: 0.6475\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3716409600.0000 - r2: 0.8420 - val_loss: 10761810944.0000 - val_r2: 0.6051\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3707588864.0000 - r2: 0.8427 - val_loss: 10352261120.0000 - val_r2: 0.6209\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3698672640.0000 - r2: 0.8427 - val_loss: 10635817984.0000 - val_r2: 0.6098\n",
      "session cleared!\n",
      "\n",
      "ix 7 i 1\n",
      "updated temp_vec [1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "going through feature_mask [1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 80175898624.0000 - r2: -2.4489 - val_loss: 17686804480.0000 - val_r2: 0.3576\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9343433728.0000 - r2: 0.6052 - val_loss: 15319055360.0000 - val_r2: 0.4409\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8208274944.0000 - r2: 0.6521 - val_loss: 13705702400.0000 - val_r2: 0.4997\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7718840320.0000 - r2: 0.6728 - val_loss: 14526059520.0000 - val_r2: 0.4692\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7370313216.0000 - r2: 0.6875 - val_loss: 13381168128.0000 - val_r2: 0.5105\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7057735680.0000 - r2: 0.7008 - val_loss: 13255412736.0000 - val_r2: 0.5152\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6746699776.0000 - r2: 0.7139 - val_loss: 12857327616.0000 - val_r2: 0.5298\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6439802880.0000 - r2: 0.7269 - val_loss: 12509320192.0000 - val_r2: 0.5424\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6150098944.0000 - r2: 0.7392 - val_loss: 11966364672.0000 - val_r2: 0.5620\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5886361600.0000 - r2: 0.7502 - val_loss: 12451021824.0000 - val_r2: 0.5452\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5653027840.0000 - r2: 0.7601 - val_loss: 11882528768.0000 - val_r2: 0.5642\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5446935040.0000 - r2: 0.7687 - val_loss: 11150854144.0000 - val_r2: 0.5916\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5261761024.0000 - r2: 0.7766 - val_loss: 11284672512.0000 - val_r2: 0.5868\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5096023040.0000 - r2: 0.7835 - val_loss: 11181058048.0000 - val_r2: 0.5902\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4948155904.0000 - r2: 0.7898 - val_loss: 11170469888.0000 - val_r2: 0.5905\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4822238208.0000 - r2: 0.7952 - val_loss: 10289341440.0000 - val_r2: 0.6230\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4719842304.0000 - r2: 0.7999 - val_loss: 10911794176.0000 - val_r2: 0.5999\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4632202752.0000 - r2: 0.8030 - val_loss: 10720440320.0000 - val_r2: 0.6076\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4564213760.0000 - r2: 0.8061 - val_loss: 10578983936.0000 - val_r2: 0.6126\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4511161856.0000 - r2: 0.8080 - val_loss: 10754480128.0000 - val_r2: 0.6061\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4468314112.0000 - r2: 0.8102 - val_loss: 10484645888.0000 - val_r2: 0.6163\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4433625088.0000 - r2: 0.8117 - val_loss: 10072026112.0000 - val_r2: 0.6306\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4407047168.0000 - r2: 0.8129 - val_loss: 10943974400.0000 - val_r2: 0.5989\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4392254464.0000 - r2: 0.8136 - val_loss: 9771290624.0000 - val_r2: 0.6421\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4374901248.0000 - r2: 0.8139 - val_loss: 10695561216.0000 - val_r2: 0.6082\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4360040448.0000 - r2: 0.8148 - val_loss: 10487298048.0000 - val_r2: 0.6151\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4348233216.0000 - r2: 0.8150 - val_loss: 10487799808.0000 - val_r2: 0.6149\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4345607168.0000 - r2: 0.8153 - val_loss: 10659180544.0000 - val_r2: 0.6095\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4334695424.0000 - r2: 0.8160 - val_loss: 10673411072.0000 - val_r2: 0.6089\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4328178176.0000 - r2: 0.8160 - val_loss: 10705417216.0000 - val_r2: 0.6070\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4330211328.0000 - r2: 0.8156 - val_loss: 10670054400.0000 - val_r2: 0.6086\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4323808256.0000 - r2: 0.8163 - val_loss: 10939929600.0000 - val_r2: 0.5997\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4312432128.0000 - r2: 0.8170 - val_loss: 11187687424.0000 - val_r2: 0.5902\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4311614976.0000 - r2: 0.8169 - val_loss: 10751865856.0000 - val_r2: 0.6061\n",
      "session cleared!\n",
      "\n",
      "ix 8 i 1\n",
      "updated temp_vec [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "going through feature_mask [1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 80158990336.0000 - r2: -2.4264 - val_loss: 18186868736.0000 - val_r2: 0.3386\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9509337088.0000 - r2: 0.5982 - val_loss: 15857568768.0000 - val_r2: 0.4209\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8439908352.0000 - r2: 0.6425 - val_loss: 14305497088.0000 - val_r2: 0.4767\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8023572480.0000 - r2: 0.6601 - val_loss: 14619164672.0000 - val_r2: 0.4651\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7759451648.0000 - r2: 0.6710 - val_loss: 13963573248.0000 - val_r2: 0.4889\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7541074432.0000 - r2: 0.6799 - val_loss: 14192051200.0000 - val_r2: 0.4811\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7342496256.0000 - r2: 0.6885 - val_loss: 13567130624.0000 - val_r2: 0.5036\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7158838272.0000 - r2: 0.6968 - val_loss: 13145064448.0000 - val_r2: 0.5195\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7000005120.0000 - r2: 0.7028 - val_loss: 13536392192.0000 - val_r2: 0.5051\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6871961600.0000 - r2: 0.7081 - val_loss: 13212436480.0000 - val_r2: 0.5155\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6772603904.0000 - r2: 0.7126 - val_loss: 13197639680.0000 - val_r2: 0.5166\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6711330304.0000 - r2: 0.7149 - val_loss: 12575580160.0000 - val_r2: 0.5401\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6662393344.0000 - r2: 0.7168 - val_loss: 13257688064.0000 - val_r2: 0.5150\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6632907264.0000 - r2: 0.7178 - val_loss: 12876764160.0000 - val_r2: 0.5290\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6611277824.0000 - r2: 0.7193 - val_loss: 13708978176.0000 - val_r2: 0.4981\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6593835520.0000 - r2: 0.7201 - val_loss: 13261856768.0000 - val_r2: 0.5148\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6588242432.0000 - r2: 0.7207 - val_loss: 13035227136.0000 - val_r2: 0.5234\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6570256384.0000 - r2: 0.7209 - val_loss: 12478478336.0000 - val_r2: 0.5427\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6559140864.0000 - r2: 0.7212 - val_loss: 13271840768.0000 - val_r2: 0.5146\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6547275776.0000 - r2: 0.7215 - val_loss: 13454249984.0000 - val_r2: 0.5071\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6545554432.0000 - r2: 0.7225 - val_loss: 13153554432.0000 - val_r2: 0.5190\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6538350592.0000 - r2: 0.7226 - val_loss: 13555025920.0000 - val_r2: 0.5046\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6527509504.0000 - r2: 0.7230 - val_loss: 12778303488.0000 - val_r2: 0.5325\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6525082112.0000 - r2: 0.7230 - val_loss: 13613329408.0000 - val_r2: 0.5013\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6513960448.0000 - r2: 0.7231 - val_loss: 13027079168.0000 - val_r2: 0.5234\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6505930752.0000 - r2: 0.7233 - val_loss: 13163394048.0000 - val_r2: 0.5175\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6498365440.0000 - r2: 0.7240 - val_loss: 12059819008.0000 - val_r2: 0.5592\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6486931456.0000 - r2: 0.7248 - val_loss: 12951625728.0000 - val_r2: 0.5253\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6486639104.0000 - r2: 0.7241 - val_loss: 12056755200.0000 - val_r2: 0.5589\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6483221504.0000 - r2: 0.7244 - val_loss: 13351948288.0000 - val_r2: 0.5118\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6475042816.0000 - r2: 0.7256 - val_loss: 14420697088.0000 - val_r2: 0.4729\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6466921984.0000 - r2: 0.7252 - val_loss: 12679285760.0000 - val_r2: 0.5358\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6461721600.0000 - r2: 0.7261 - val_loss: 13083009024.0000 - val_r2: 0.5209\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6455377920.0000 - r2: 0.7261 - val_loss: 12688973824.0000 - val_r2: 0.5356\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6450290688.0000 - r2: 0.7256 - val_loss: 13568328704.0000 - val_r2: 0.5036\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6445832704.0000 - r2: 0.7267 - val_loss: 13505464320.0000 - val_r2: 0.5055\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6437641728.0000 - r2: 0.7269 - val_loss: 13232326656.0000 - val_r2: 0.5149\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6441622016.0000 - r2: 0.7272 - val_loss: 13128293376.0000 - val_r2: 0.5193\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6431753728.0000 - r2: 0.7268 - val_loss: 13192140800.0000 - val_r2: 0.5168\n",
      "session cleared!\n",
      "\n",
      "570.1791179180145 seconds elapsed\n",
      "\n",
      "vec [0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "ix 0 i 0\n",
      "ix 1 i 1\n",
      "updated temp_vec [0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 90005692416.0000 - r2: -2.8329 - val_loss: 19830272000.0000 - val_r2: 0.2789\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 10645669888.0000 - r2: 0.5503 - val_loss: 16980983808.0000 - val_r2: 0.3818\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9456443392.0000 - r2: 0.6001 - val_loss: 15808874496.0000 - val_r2: 0.4234\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8786861056.0000 - r2: 0.6282 - val_loss: 15212924928.0000 - val_r2: 0.4443\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8235969536.0000 - r2: 0.6511 - val_loss: 14868723712.0000 - val_r2: 0.4574\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7713276416.0000 - r2: 0.6736 - val_loss: 14186053632.0000 - val_r2: 0.4818\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7206021632.0000 - r2: 0.6948 - val_loss: 14747133952.0000 - val_r2: 0.4616\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6742014464.0000 - r2: 0.7149 - val_loss: 13171303424.0000 - val_r2: 0.5188\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6348805632.0000 - r2: 0.7311 - val_loss: 12585592832.0000 - val_r2: 0.5401\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6012047360.0000 - r2: 0.7457 - val_loss: 12634760192.0000 - val_r2: 0.5385\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5736128512.0000 - r2: 0.7566 - val_loss: 11935260672.0000 - val_r2: 0.5642\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 5500752896.0000 - r2: 0.7672 - val_loss: 12131320832.0000 - val_r2: 0.5564\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 5289134080.0000 - r2: 0.7759 - val_loss: 12078824448.0000 - val_r2: 0.5578\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5111194112.0000 - r2: 0.7834 - val_loss: 11510254592.0000 - val_r2: 0.5787\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4957054464.0000 - r2: 0.7899 - val_loss: 11813090304.0000 - val_r2: 0.5675\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4831053312.0000 - r2: 0.7953 - val_loss: 11820143616.0000 - val_r2: 0.5670\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4727578112.0000 - r2: 0.7997 - val_loss: 11686343680.0000 - val_r2: 0.5720\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4644462592.0000 - r2: 0.8026 - val_loss: 11796517888.0000 - val_r2: 0.5681\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4582528512.0000 - r2: 0.8055 - val_loss: 11471808512.0000 - val_r2: 0.5799\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4533689344.0000 - r2: 0.8075 - val_loss: 10889774080.0000 - val_r2: 0.6016\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4493766656.0000 - r2: 0.8095 - val_loss: 11653874688.0000 - val_r2: 0.5736\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4462586368.0000 - r2: 0.8108 - val_loss: 11904147456.0000 - val_r2: 0.5639\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4438259712.0000 - r2: 0.8115 - val_loss: 10838308864.0000 - val_r2: 0.6034\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4422277120.0000 - r2: 0.8125 - val_loss: 10559202304.0000 - val_r2: 0.6137\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4409693184.0000 - r2: 0.8129 - val_loss: 10907449344.0000 - val_r2: 0.6005\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4394734080.0000 - r2: 0.8137 - val_loss: 11791185920.0000 - val_r2: 0.5685\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4388009984.0000 - r2: 0.8139 - val_loss: 12077436928.0000 - val_r2: 0.5580\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4381473792.0000 - r2: 0.8141 - val_loss: 10609126400.0000 - val_r2: 0.6119\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4377974272.0000 - r2: 0.8143 - val_loss: 11426770944.0000 - val_r2: 0.5809\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4369295360.0000 - r2: 0.8143 - val_loss: 10923433984.0000 - val_r2: 0.5998\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4363771904.0000 - r2: 0.8148 - val_loss: 10503582720.0000 - val_r2: 0.6155\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4361182208.0000 - r2: 0.8146 - val_loss: 11285430272.0000 - val_r2: 0.5869\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4349387264.0000 - r2: 0.8152 - val_loss: 10613336064.0000 - val_r2: 0.6121\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4352168960.0000 - r2: 0.8154 - val_loss: 10952532992.0000 - val_r2: 0.5978\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4344773632.0000 - r2: 0.8155 - val_loss: 10671338496.0000 - val_r2: 0.6091\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4346535424.0000 - r2: 0.8156 - val_loss: 10471223296.0000 - val_r2: 0.6166\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4341458432.0000 - r2: 0.8158 - val_loss: 11071927296.0000 - val_r2: 0.5945\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4340793856.0000 - r2: 0.8155 - val_loss: 10328383488.0000 - val_r2: 0.6221\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4342792192.0000 - r2: 0.8158 - val_loss: 10833520640.0000 - val_r2: 0.6036\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4330809344.0000 - r2: 0.8162 - val_loss: 11316944896.0000 - val_r2: 0.5860\n",
      "Epoch 41/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4330205184.0000 - r2: 0.8165 - val_loss: 11302493184.0000 - val_r2: 0.5867\n",
      "Epoch 42/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4331473920.0000 - r2: 0.8160 - val_loss: 11888680960.0000 - val_r2: 0.5645\n",
      "Epoch 43/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4328078336.0000 - r2: 0.8163 - val_loss: 10934825984.0000 - val_r2: 0.6002\n",
      "Epoch 44/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4328529408.0000 - r2: 0.8165 - val_loss: 11404837888.0000 - val_r2: 0.5815\n",
      "Epoch 45/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4325100032.0000 - r2: 0.8165 - val_loss: 11374733312.0000 - val_r2: 0.5834\n",
      "Epoch 46/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4325207552.0000 - r2: 0.8167 - val_loss: 11079190528.0000 - val_r2: 0.5942\n",
      "Epoch 47/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4323763200.0000 - r2: 0.8161 - val_loss: 11451700224.0000 - val_r2: 0.5811\n",
      "Epoch 48/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4325681152.0000 - r2: 0.8161 - val_loss: 11125625856.0000 - val_r2: 0.5930\n",
      "session cleared!\n",
      "\n",
      "ix 2 i 1\n",
      "updated temp_vec [0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [0, 1, 0, 1, 1, 1, 1, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 116803567616.0000 - r2: -3.8894 - val_loss: 32792778752.0000 - val_r2: -0.1930\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 19393314816.0000 - r2: 0.1809 - val_loss: 27624755200.0000 - val_r2: -0.0074\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 17267003392.0000 - r2: 0.2699 - val_loss: 25910161408.0000 - val_r2: 0.0533\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 15947011072.0000 - r2: 0.3255 - val_loss: 24429602816.0000 - val_r2: 0.1070\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 14702606336.0000 - r2: 0.3771 - val_loss: 22537934848.0000 - val_r2: 0.1766\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 1s 3ms/step - loss: 13371957248.0000 - r2: 0.4340 - val_loss: 20977225728.0000 - val_r2: 0.2328\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 1s 3ms/step - loss: 11929003008.0000 - r2: 0.4947 - val_loss: 19830919168.0000 - val_r2: 0.2747\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 1s 3ms/step - loss: 10387715072.0000 - r2: 0.5598 - val_loss: 18092353536.0000 - val_r2: 0.3380\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 1s 3ms/step - loss: 8844957696.0000 - r2: 0.6255 - val_loss: 16083867648.0000 - val_r2: 0.4116\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 1s 3ms/step - loss: 7434096640.0000 - r2: 0.6852 - val_loss: 14625618944.0000 - val_r2: 0.4654\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 6271909376.0000 - r2: 0.7343 - val_loss: 13743147008.0000 - val_r2: 0.4963\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 5434492928.0000 - r2: 0.7703 - val_loss: 11707303936.0000 - val_r2: 0.5719\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4887925760.0000 - r2: 0.7927 - val_loss: 12021276672.0000 - val_r2: 0.5603\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4562963968.0000 - r2: 0.8063 - val_loss: 11130519552.0000 - val_r2: 0.5921\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4398455808.0000 - r2: 0.8135 - val_loss: 11286612992.0000 - val_r2: 0.5865\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4334199296.0000 - r2: 0.8163 - val_loss: 11304624128.0000 - val_r2: 0.5861\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4309671936.0000 - r2: 0.8170 - val_loss: 11033714688.0000 - val_r2: 0.5968\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4300701696.0000 - r2: 0.8173 - val_loss: 11010307072.0000 - val_r2: 0.5974\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4299910144.0000 - r2: 0.8175 - val_loss: 11107769344.0000 - val_r2: 0.5934\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4302593024.0000 - r2: 0.8176 - val_loss: 11152720896.0000 - val_r2: 0.5911\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4299191296.0000 - r2: 0.8176 - val_loss: 11130767360.0000 - val_r2: 0.5920\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4299333632.0000 - r2: 0.8171 - val_loss: 11135420416.0000 - val_r2: 0.5916\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4302606336.0000 - r2: 0.8175 - val_loss: 11209032704.0000 - val_r2: 0.5894\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4298905088.0000 - r2: 0.8171 - val_loss: 10031071232.0000 - val_r2: 0.6327\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4302756864.0000 - r2: 0.8169 - val_loss: 10651707392.0000 - val_r2: 0.6099\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4299097600.0000 - r2: 0.8176 - val_loss: 10921047040.0000 - val_r2: 0.5998\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4299481088.0000 - r2: 0.8177 - val_loss: 10554843136.0000 - val_r2: 0.6136\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4301577728.0000 - r2: 0.8170 - val_loss: 10606294016.0000 - val_r2: 0.6117\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4301124608.0000 - r2: 0.8174 - val_loss: 10266842112.0000 - val_r2: 0.6244\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4299602944.0000 - r2: 0.8177 - val_loss: 11022340096.0000 - val_r2: 0.5967\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4301990912.0000 - r2: 0.8170 - val_loss: 10816647168.0000 - val_r2: 0.6040\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4300320256.0000 - r2: 0.8173 - val_loss: 11427849216.0000 - val_r2: 0.5821\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4300383232.0000 - r2: 0.8175 - val_loss: 10913975296.0000 - val_r2: 0.6008\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4301914624.0000 - r2: 0.8173 - val_loss: 10776312832.0000 - val_r2: 0.6051\n",
      "session cleared!\n",
      "\n",
      "ix 3 i 1\n",
      "updated temp_vec [0, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83839279104.0000 - r2: -2.5474 - val_loss: 18059087872.0000 - val_r2: 0.3430\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9215245312.0000 - r2: 0.6107 - val_loss: 14565794816.0000 - val_r2: 0.4679\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7917755904.0000 - r2: 0.6648 - val_loss: 13702408192.0000 - val_r2: 0.4997\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7315540480.0000 - r2: 0.6902 - val_loss: 13301528576.0000 - val_r2: 0.5133\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6872672768.0000 - r2: 0.7086 - val_loss: 12730893312.0000 - val_r2: 0.5343\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6473196544.0000 - r2: 0.7256 - val_loss: 12524326912.0000 - val_r2: 0.5419\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6090057728.0000 - r2: 0.7415 - val_loss: 11606441984.0000 - val_r2: 0.5754\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5742655488.0000 - r2: 0.7563 - val_loss: 11456474112.0000 - val_r2: 0.5813\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5437439488.0000 - r2: 0.7691 - val_loss: 12078303232.0000 - val_r2: 0.5574\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5182807040.0000 - r2: 0.7795 - val_loss: 11332683776.0000 - val_r2: 0.5849\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4981968896.0000 - r2: 0.7883 - val_loss: 11260435456.0000 - val_r2: 0.5869\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4804116480.0000 - r2: 0.7961 - val_loss: 11685789696.0000 - val_r2: 0.5720\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4656887808.0000 - r2: 0.8024 - val_loss: 11002479616.0000 - val_r2: 0.5977\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4522317824.0000 - r2: 0.8081 - val_loss: 10670326784.0000 - val_r2: 0.6092\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4413960704.0000 - r2: 0.8129 - val_loss: 9852830720.0000 - val_r2: 0.6388\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4317558784.0000 - r2: 0.8168 - val_loss: 10492532736.0000 - val_r2: 0.6160\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4245321472.0000 - r2: 0.8199 - val_loss: 10460804096.0000 - val_r2: 0.6159\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4185410048.0000 - r2: 0.8224 - val_loss: 10313739264.0000 - val_r2: 0.6217\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4138877440.0000 - r2: 0.8242 - val_loss: 10554099712.0000 - val_r2: 0.6125\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4104815104.0000 - r2: 0.8254 - val_loss: 11189902336.0000 - val_r2: 0.5895\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4080284928.0000 - r2: 0.8267 - val_loss: 10401379328.0000 - val_r2: 0.6185\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4066540288.0000 - r2: 0.8271 - val_loss: 10268734464.0000 - val_r2: 0.6234\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4050216960.0000 - r2: 0.8283 - val_loss: 10772934656.0000 - val_r2: 0.6048\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4046470656.0000 - r2: 0.8281 - val_loss: 9828080640.0000 - val_r2: 0.6398\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4038330624.0000 - r2: 0.8283 - val_loss: 9471824896.0000 - val_r2: 0.6529\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4040138752.0000 - r2: 0.8283 - val_loss: 10334856192.0000 - val_r2: 0.6211\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4034928640.0000 - r2: 0.8286 - val_loss: 9885783040.0000 - val_r2: 0.6379\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4033774336.0000 - r2: 0.8286 - val_loss: 10558016512.0000 - val_r2: 0.6136\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4033547008.0000 - r2: 0.8288 - val_loss: 9963298816.0000 - val_r2: 0.6345\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4037455872.0000 - r2: 0.8285 - val_loss: 10527281152.0000 - val_r2: 0.6132\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4030567936.0000 - r2: 0.8283 - val_loss: 10201422848.0000 - val_r2: 0.6253\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4034401792.0000 - r2: 0.8288 - val_loss: 10520246272.0000 - val_r2: 0.6146\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4033681664.0000 - r2: 0.8287 - val_loss: 9845316608.0000 - val_r2: 0.6395\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4034972928.0000 - r2: 0.8283 - val_loss: 10765573120.0000 - val_r2: 0.6061\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4037021696.0000 - r2: 0.8284 - val_loss: 10487278592.0000 - val_r2: 0.6155\n",
      "session cleared!\n",
      "\n",
      "ix 4 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83386630144.0000 - r2: -2.5483 - val_loss: 18228957184.0000 - val_r2: 0.3373\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9244055552.0000 - r2: 0.6092 - val_loss: 14702411776.0000 - val_r2: 0.4637\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7956681216.0000 - r2: 0.6625 - val_loss: 14026158080.0000 - val_r2: 0.4872\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7359891456.0000 - r2: 0.6881 - val_loss: 13720023040.0000 - val_r2: 0.4983\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6937421824.0000 - r2: 0.7057 - val_loss: 13204143104.0000 - val_r2: 0.5166\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6560001536.0000 - r2: 0.7214 - val_loss: 12539025408.0000 - val_r2: 0.5411\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6207460352.0000 - r2: 0.7366 - val_loss: 12199351296.0000 - val_r2: 0.5538\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5896666112.0000 - r2: 0.7497 - val_loss: 11943270400.0000 - val_r2: 0.5620\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5620384256.0000 - r2: 0.7616 - val_loss: 11452865536.0000 - val_r2: 0.5801\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5415740416.0000 - r2: 0.7703 - val_loss: 11272138752.0000 - val_r2: 0.5875\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5234055168.0000 - r2: 0.7780 - val_loss: 11854760960.0000 - val_r2: 0.5655\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5088158208.0000 - r2: 0.7841 - val_loss: 11008510976.0000 - val_r2: 0.5964\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4958414336.0000 - r2: 0.7895 - val_loss: 11427966976.0000 - val_r2: 0.5811\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4840226816.0000 - r2: 0.7947 - val_loss: 11308126208.0000 - val_r2: 0.5859\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4735406080.0000 - r2: 0.7987 - val_loss: 10650008576.0000 - val_r2: 0.6105\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4650805760.0000 - r2: 0.8023 - val_loss: 10082204672.0000 - val_r2: 0.6305\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4571046912.0000 - r2: 0.8060 - val_loss: 10719252480.0000 - val_r2: 0.6070\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4509652992.0000 - r2: 0.8084 - val_loss: 11150828544.0000 - val_r2: 0.5917\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4468514304.0000 - r2: 0.8104 - val_loss: 11979574272.0000 - val_r2: 0.5610\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4432396288.0000 - r2: 0.8114 - val_loss: 10930905088.0000 - val_r2: 0.6001\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4406353920.0000 - r2: 0.8130 - val_loss: 10471836672.0000 - val_r2: 0.6159\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4386549760.0000 - r2: 0.8138 - val_loss: 10295397376.0000 - val_r2: 0.6228\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4373530624.0000 - r2: 0.8143 - val_loss: 10845365248.0000 - val_r2: 0.6031\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4370723328.0000 - r2: 0.8143 - val_loss: 11147707392.0000 - val_r2: 0.5918\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4364005376.0000 - r2: 0.8146 - val_loss: 10876574720.0000 - val_r2: 0.6005\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4358935040.0000 - r2: 0.8152 - val_loss: 10326127616.0000 - val_r2: 0.6212\n",
      "session cleared!\n",
      "\n",
      "ix 5 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83965829120.0000 - r2: -2.5575 - val_loss: 18129956864.0000 - val_r2: 0.3411\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9331533824.0000 - r2: 0.6053 - val_loss: 14852483072.0000 - val_r2: 0.4578\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8068550144.0000 - r2: 0.6578 - val_loss: 14285746176.0000 - val_r2: 0.4782\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7510617088.0000 - r2: 0.6816 - val_loss: 13828570112.0000 - val_r2: 0.4943\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7106737152.0000 - r2: 0.6986 - val_loss: 12747389952.0000 - val_r2: 0.5337\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6732551168.0000 - r2: 0.7144 - val_loss: 12686903296.0000 - val_r2: 0.5364\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6354995200.0000 - r2: 0.7310 - val_loss: 11971852288.0000 - val_r2: 0.5625\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5990754304.0000 - r2: 0.7456 - val_loss: 11666651136.0000 - val_r2: 0.5733\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5648307200.0000 - r2: 0.7600 - val_loss: 11520303104.0000 - val_r2: 0.5790\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5345218048.0000 - r2: 0.7736 - val_loss: 11461672960.0000 - val_r2: 0.5808\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5090892288.0000 - r2: 0.7841 - val_loss: 11498575872.0000 - val_r2: 0.5791\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4879182336.0000 - r2: 0.7926 - val_loss: 11792990208.0000 - val_r2: 0.5682\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4699447296.0000 - r2: 0.8007 - val_loss: 11322973184.0000 - val_r2: 0.5849\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4544199680.0000 - r2: 0.8072 - val_loss: 10204312576.0000 - val_r2: 0.6266\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4405832704.0000 - r2: 0.8130 - val_loss: 10222858240.0000 - val_r2: 0.6256\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4294457600.0000 - r2: 0.8178 - val_loss: 10494285824.0000 - val_r2: 0.6153\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4191257600.0000 - r2: 0.8221 - val_loss: 9946973184.0000 - val_r2: 0.6358\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4110819328.0000 - r2: 0.8250 - val_loss: 10996540416.0000 - val_r2: 0.5975\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4048609024.0000 - r2: 0.8277 - val_loss: 11035852800.0000 - val_r2: 0.5954\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3999833856.0000 - r2: 0.8303 - val_loss: 9633631232.0000 - val_r2: 0.6475\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3962371328.0000 - r2: 0.8315 - val_loss: 10580331520.0000 - val_r2: 0.6128\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3933795840.0000 - r2: 0.8329 - val_loss: 10002265088.0000 - val_r2: 0.6335\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3908149248.0000 - r2: 0.8341 - val_loss: 10993312768.0000 - val_r2: 0.5971\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3891889664.0000 - r2: 0.8349 - val_loss: 10630715392.0000 - val_r2: 0.6096\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3879041280.0000 - r2: 0.8352 - val_loss: 10116176896.0000 - val_r2: 0.6290\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3867250688.0000 - r2: 0.8357 - val_loss: 9833509888.0000 - val_r2: 0.6392\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3858376448.0000 - r2: 0.8363 - val_loss: 10235920384.0000 - val_r2: 0.6246\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3849780480.0000 - r2: 0.8366 - val_loss: 9243064320.0000 - val_r2: 0.6607\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3843236096.0000 - r2: 0.8368 - val_loss: 10207884288.0000 - val_r2: 0.6261\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3836257024.0000 - r2: 0.8370 - val_loss: 10488264704.0000 - val_r2: 0.6142\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3827999232.0000 - r2: 0.8374 - val_loss: 10145711104.0000 - val_r2: 0.6281\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3825127680.0000 - r2: 0.8377 - val_loss: 10550017024.0000 - val_r2: 0.6129\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3819906048.0000 - r2: 0.8375 - val_loss: 9825393664.0000 - val_r2: 0.6388\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3816203776.0000 - r2: 0.8380 - val_loss: 11314226176.0000 - val_r2: 0.5848\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3814184448.0000 - r2: 0.8378 - val_loss: 10099957760.0000 - val_r2: 0.6297\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3809821440.0000 - r2: 0.8378 - val_loss: 9941312512.0000 - val_r2: 0.6360\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3802808320.0000 - r2: 0.8382 - val_loss: 10479225856.0000 - val_r2: 0.6159\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3800049920.0000 - r2: 0.8384 - val_loss: 10546983936.0000 - val_r2: 0.6136\n",
      "new min loss: len 8, ix 5\n",
      "session cleared!\n",
      "\n",
      "ix 6 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83593543680.0000 - r2: -2.5541 - val_loss: 18029369344.0000 - val_r2: 0.3444\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9345782784.0000 - r2: 0.6053 - val_loss: 14858757120.0000 - val_r2: 0.4582\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8115495424.0000 - r2: 0.6558 - val_loss: 13958162432.0000 - val_r2: 0.4899\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7563288064.0000 - r2: 0.6796 - val_loss: 14024923136.0000 - val_r2: 0.4878\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7156594688.0000 - r2: 0.6967 - val_loss: 13018455040.0000 - val_r2: 0.5236\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6780131328.0000 - r2: 0.7125 - val_loss: 13190541312.0000 - val_r2: 0.5182\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6389032448.0000 - r2: 0.7292 - val_loss: 12234297344.0000 - val_r2: 0.5524\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5998474752.0000 - r2: 0.7455 - val_loss: 11957411840.0000 - val_r2: 0.5630\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5604696064.0000 - r2: 0.7622 - val_loss: 11831913472.0000 - val_r2: 0.5671\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5246516224.0000 - r2: 0.7776 - val_loss: 11684023296.0000 - val_r2: 0.5715\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4932328960.0000 - r2: 0.7907 - val_loss: 10909723648.0000 - val_r2: 0.6010\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4656014848.0000 - r2: 0.8025 - val_loss: 10421347328.0000 - val_r2: 0.6185\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4422565888.0000 - r2: 0.8124 - val_loss: 10325799936.0000 - val_r2: 0.6226\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4231491840.0000 - r2: 0.8206 - val_loss: 10099499008.0000 - val_r2: 0.6307\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4084504576.0000 - r2: 0.8261 - val_loss: 10966948864.0000 - val_r2: 0.5984\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3977910784.0000 - r2: 0.8310 - val_loss: 10167856128.0000 - val_r2: 0.6278\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3910802176.0000 - r2: 0.8339 - val_loss: 10716382208.0000 - val_r2: 0.6070\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3867871744.0000 - r2: 0.8357 - val_loss: 10241861632.0000 - val_r2: 0.6249\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3842390016.0000 - r2: 0.8371 - val_loss: 9173248000.0000 - val_r2: 0.6642\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3818849280.0000 - r2: 0.8377 - val_loss: 9983319040.0000 - val_r2: 0.6339\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3805117440.0000 - r2: 0.8385 - val_loss: 9759128576.0000 - val_r2: 0.6422\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3797113600.0000 - r2: 0.8385 - val_loss: 10103607296.0000 - val_r2: 0.6294\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3783937536.0000 - r2: 0.8391 - val_loss: 9649896448.0000 - val_r2: 0.6462\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3777030912.0000 - r2: 0.8394 - val_loss: 11088484352.0000 - val_r2: 0.5935\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3764782336.0000 - r2: 0.8401 - val_loss: 10083856384.0000 - val_r2: 0.6310\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3755526656.0000 - r2: 0.8404 - val_loss: 10057212928.0000 - val_r2: 0.6310\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3745123840.0000 - r2: 0.8412 - val_loss: 9981505536.0000 - val_r2: 0.6345\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3743236608.0000 - r2: 0.8408 - val_loss: 9707754496.0000 - val_r2: 0.6442\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3735941376.0000 - r2: 0.8411 - val_loss: 10340207616.0000 - val_r2: 0.6211\n",
      "new min loss: len 8, ix 6\n",
      "session cleared!\n",
      "\n",
      "ix 7 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83564371968.0000 - r2: -2.5378 - val_loss: 18459748352.0000 - val_r2: 0.3285\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9405271040.0000 - r2: 0.6025 - val_loss: 15149900800.0000 - val_r2: 0.4472\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8183810560.0000 - r2: 0.6533 - val_loss: 14010522624.0000 - val_r2: 0.4882\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7662575104.0000 - r2: 0.6746 - val_loss: 14219698176.0000 - val_r2: 0.4796\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7307174400.0000 - r2: 0.6900 - val_loss: 13007501312.0000 - val_r2: 0.5238\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6988192768.0000 - r2: 0.7033 - val_loss: 12865484800.0000 - val_r2: 0.5293\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6675596288.0000 - r2: 0.7170 - val_loss: 12723016704.0000 - val_r2: 0.5338\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6372205056.0000 - r2: 0.7297 - val_loss: 12116942848.0000 - val_r2: 0.5565\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6087164928.0000 - r2: 0.7416 - val_loss: 11905811456.0000 - val_r2: 0.5647\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5833146368.0000 - r2: 0.7526 - val_loss: 12113833984.0000 - val_r2: 0.5567\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5606953472.0000 - r2: 0.7622 - val_loss: 11815682048.0000 - val_r2: 0.5676\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5405927936.0000 - r2: 0.7703 - val_loss: 11153462272.0000 - val_r2: 0.5916\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5224197120.0000 - r2: 0.7780 - val_loss: 11240827904.0000 - val_r2: 0.5881\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5061809152.0000 - r2: 0.7854 - val_loss: 10664211456.0000 - val_r2: 0.6092\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4926023168.0000 - r2: 0.7905 - val_loss: 11253691392.0000 - val_r2: 0.5874\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4803828224.0000 - r2: 0.7957 - val_loss: 10995560448.0000 - val_r2: 0.5974\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4703695360.0000 - r2: 0.8003 - val_loss: 11166728192.0000 - val_r2: 0.5907\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4618240000.0000 - r2: 0.8041 - val_loss: 10789804032.0000 - val_r2: 0.6045\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4556971520.0000 - r2: 0.8066 - val_loss: 10069328896.0000 - val_r2: 0.6309\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4506068480.0000 - r2: 0.8084 - val_loss: 10604805120.0000 - val_r2: 0.6123\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4463435776.0000 - r2: 0.8104 - val_loss: 10827491328.0000 - val_r2: 0.6030\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4432708608.0000 - r2: 0.8119 - val_loss: 9912639488.0000 - val_r2: 0.6376\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4410599424.0000 - r2: 0.8123 - val_loss: 10344053760.0000 - val_r2: 0.6207\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4389508096.0000 - r2: 0.8134 - val_loss: 10625531904.0000 - val_r2: 0.6104\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4375076352.0000 - r2: 0.8140 - val_loss: 10398173184.0000 - val_r2: 0.6185\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4363612160.0000 - r2: 0.8145 - val_loss: 10096433152.0000 - val_r2: 0.6303\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4354134528.0000 - r2: 0.8150 - val_loss: 10378398720.0000 - val_r2: 0.6202\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4346710528.0000 - r2: 0.8152 - val_loss: 9848462336.0000 - val_r2: 0.6390\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4336858624.0000 - r2: 0.8158 - val_loss: 11493597184.0000 - val_r2: 0.5787\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4333722624.0000 - r2: 0.8158 - val_loss: 10486359040.0000 - val_r2: 0.6161\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4334154752.0000 - r2: 0.8156 - val_loss: 10557164544.0000 - val_r2: 0.6129\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4323890688.0000 - r2: 0.8163 - val_loss: 10667595776.0000 - val_r2: 0.6091\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4325086720.0000 - r2: 0.8162 - val_loss: 10104910848.0000 - val_r2: 0.6298\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4315066880.0000 - r2: 0.8169 - val_loss: 10421852160.0000 - val_r2: 0.6182\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4310144000.0000 - r2: 0.8167 - val_loss: 10259040256.0000 - val_r2: 0.6243\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4307928576.0000 - r2: 0.8165 - val_loss: 10766547968.0000 - val_r2: 0.6056\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4311762432.0000 - r2: 0.8168 - val_loss: 10457050112.0000 - val_r2: 0.6165\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4305048576.0000 - r2: 0.8169 - val_loss: 10685855744.0000 - val_r2: 0.6084\n",
      "session cleared!\n",
      "\n",
      "ix 8 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 84165976064.0000 - r2: -2.5502 - val_loss: 18399973376.0000 - val_r2: 0.3308\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9584312320.0000 - r2: 0.5947 - val_loss: 14980865024.0000 - val_r2: 0.4531\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8436067328.0000 - r2: 0.6420 - val_loss: 14372782080.0000 - val_r2: 0.4752\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8002133504.0000 - r2: 0.6611 - val_loss: 14090211328.0000 - val_r2: 0.4847\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7735772672.0000 - r2: 0.6719 - val_loss: 13578053632.0000 - val_r2: 0.5033\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7512758784.0000 - r2: 0.6812 - val_loss: 13132097536.0000 - val_r2: 0.5197\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7315742720.0000 - r2: 0.6898 - val_loss: 13815695360.0000 - val_r2: 0.4939\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7135713792.0000 - r2: 0.6976 - val_loss: 12684075008.0000 - val_r2: 0.5357\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6983661568.0000 - r2: 0.7040 - val_loss: 12630836224.0000 - val_r2: 0.5382\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6862861824.0000 - r2: 0.7091 - val_loss: 13249691648.0000 - val_r2: 0.5155\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6769325056.0000 - r2: 0.7131 - val_loss: 13242088448.0000 - val_r2: 0.5142\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6707618816.0000 - r2: 0.7155 - val_loss: 12123368448.0000 - val_r2: 0.5558\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6673852416.0000 - r2: 0.7172 - val_loss: 12639850496.0000 - val_r2: 0.5369\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6640315904.0000 - r2: 0.7179 - val_loss: 12784731136.0000 - val_r2: 0.5322\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6618140160.0000 - r2: 0.7189 - val_loss: 12194622464.0000 - val_r2: 0.5539\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6602052096.0000 - r2: 0.7196 - val_loss: 13485679616.0000 - val_r2: 0.5061\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6590389248.0000 - r2: 0.7205 - val_loss: 12526616576.0000 - val_r2: 0.5410\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6577262592.0000 - r2: 0.7208 - val_loss: 12338122752.0000 - val_r2: 0.5478\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6569065984.0000 - r2: 0.7211 - val_loss: 13035734016.0000 - val_r2: 0.5235\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6555034624.0000 - r2: 0.7216 - val_loss: 12353913856.0000 - val_r2: 0.5475\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6552752128.0000 - r2: 0.7222 - val_loss: 13201506304.0000 - val_r2: 0.5170\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6543037952.0000 - r2: 0.7218 - val_loss: 12778235904.0000 - val_r2: 0.5316\n",
      "session cleared!\n",
      "\n",
      "1042.2648494243622 seconds elapsed\n",
      "\n",
      "vec [0, 1, 1, 1, 1, 1, 0, 1, 1]\n",
      "ix 0 i 0\n",
      "ix 1 i 1\n",
      "updated temp_vec [0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "going through feature_mask [0, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 89987153920.0000 - r2: -2.8522 - val_loss: 19506042880.0000 - val_r2: 0.2909\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 10884641792.0000 - r2: 0.5405 - val_loss: 17683671040.0000 - val_r2: 0.3553\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9763692544.0000 - r2: 0.5875 - val_loss: 16492160000.0000 - val_r2: 0.3986\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9143046144.0000 - r2: 0.6133 - val_loss: 16088696832.0000 - val_r2: 0.4131\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8640017408.0000 - r2: 0.6341 - val_loss: 15720560640.0000 - val_r2: 0.4266\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8154136576.0000 - r2: 0.6545 - val_loss: 14086389760.0000 - val_r2: 0.4856\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7650281472.0000 - r2: 0.6765 - val_loss: 13691954176.0000 - val_r2: 0.5005\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7137410560.0000 - r2: 0.6980 - val_loss: 13684852736.0000 - val_r2: 0.5003\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6634145792.0000 - r2: 0.7191 - val_loss: 13198139392.0000 - val_r2: 0.5180\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6163918336.0000 - r2: 0.7390 - val_loss: 13091895296.0000 - val_r2: 0.5217\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5758565888.0000 - r2: 0.7563 - val_loss: 11897107456.0000 - val_r2: 0.5649\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5398583808.0000 - r2: 0.7712 - val_loss: 12058244096.0000 - val_r2: 0.5588\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5104091136.0000 - r2: 0.7842 - val_loss: 11964025856.0000 - val_r2: 0.5628\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4876264960.0000 - r2: 0.7935 - val_loss: 12084913152.0000 - val_r2: 0.5573\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4712729600.0000 - r2: 0.8001 - val_loss: 11850822656.0000 - val_r2: 0.5669\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4610267648.0000 - r2: 0.8046 - val_loss: 10278489088.0000 - val_r2: 0.6233\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4546248192.0000 - r2: 0.8071 - val_loss: 10905100288.0000 - val_r2: 0.6013\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4500249600.0000 - r2: 0.8092 - val_loss: 11305452544.0000 - val_r2: 0.5855\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4478354944.0000 - r2: 0.8098 - val_loss: 10851313664.0000 - val_r2: 0.6026\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4464063488.0000 - r2: 0.8103 - val_loss: 11783289856.0000 - val_r2: 0.5683\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4447757824.0000 - r2: 0.8113 - val_loss: 11100526592.0000 - val_r2: 0.5934\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4441122816.0000 - r2: 0.8118 - val_loss: 10469043200.0000 - val_r2: 0.6169\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4435063808.0000 - r2: 0.8120 - val_loss: 10524083200.0000 - val_r2: 0.6147\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4427213824.0000 - r2: 0.8119 - val_loss: 11046732800.0000 - val_r2: 0.5958\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4413429248.0000 - r2: 0.8127 - val_loss: 11112513536.0000 - val_r2: 0.5935\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4403909120.0000 - r2: 0.8134 - val_loss: 11298569216.0000 - val_r2: 0.5865\n",
      "session cleared!\n",
      "\n",
      "ix 2 i 1\n",
      "updated temp_vec [0, 1, 0, 1, 1, 1, 0, 1, 1]\n",
      "going through feature_mask [0, 1, 0, 1, 1, 1, 0, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 116594532352.0000 - r2: -3.9555 - val_loss: 32838334464.0000 - val_r2: -0.1952\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 19538227200.0000 - r2: 0.1750 - val_loss: 27668328448.0000 - val_r2: -0.0096\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 17444386816.0000 - r2: 0.2622 - val_loss: 25567787008.0000 - val_r2: 0.0678\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 16114204672.0000 - r2: 0.3174 - val_loss: 24116148224.0000 - val_r2: 0.1186\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 14844802048.0000 - r2: 0.3717 - val_loss: 22816520192.0000 - val_r2: 0.1667\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 13468339200.0000 - r2: 0.4290 - val_loss: 21709289472.0000 - val_r2: 0.2065\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 11946966016.0000 - r2: 0.4938 - val_loss: 19292381184.0000 - val_r2: 0.2949\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 10329523200.0000 - r2: 0.5627 - val_loss: 17666799616.0000 - val_r2: 0.3541\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8731555840.0000 - r2: 0.6306 - val_loss: 15978203136.0000 - val_r2: 0.4161\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 7287352832.0000 - r2: 0.6918 - val_loss: 14004008960.0000 - val_r2: 0.4875\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 6139458048.0000 - r2: 0.7401 - val_loss: 13113492480.0000 - val_r2: 0.5202\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 5328577024.0000 - r2: 0.7738 - val_loss: 12270082048.0000 - val_r2: 0.5509\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4824365568.0000 - r2: 0.7955 - val_loss: 11325787136.0000 - val_r2: 0.5860\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4541285888.0000 - r2: 0.8072 - val_loss: 11763961856.0000 - val_r2: 0.5690\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4400336384.0000 - r2: 0.8131 - val_loss: 10813936640.0000 - val_r2: 0.6042\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4339005440.0000 - r2: 0.8156 - val_loss: 10986704896.0000 - val_r2: 0.5980\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4317920768.0000 - r2: 0.8164 - val_loss: 10672699392.0000 - val_r2: 0.6092\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4310366208.0000 - r2: 0.8171 - val_loss: 10789411840.0000 - val_r2: 0.6039\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4308696576.0000 - r2: 0.8168 - val_loss: 11186969600.0000 - val_r2: 0.5905\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4308765696.0000 - r2: 0.8169 - val_loss: 11530619904.0000 - val_r2: 0.5774\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4313685504.0000 - r2: 0.8167 - val_loss: 11364387840.0000 - val_r2: 0.5837\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4310763520.0000 - r2: 0.8167 - val_loss: 10629494784.0000 - val_r2: 0.6106\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4311959040.0000 - r2: 0.8169 - val_loss: 11353272320.0000 - val_r2: 0.5840\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4307700224.0000 - r2: 0.8169 - val_loss: 10363502592.0000 - val_r2: 0.6207\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4307980288.0000 - r2: 0.8172 - val_loss: 11261588480.0000 - val_r2: 0.5871\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4308119552.0000 - r2: 0.8170 - val_loss: 11056994304.0000 - val_r2: 0.5944\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4307963904.0000 - r2: 0.8168 - val_loss: 10027071488.0000 - val_r2: 0.6333\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4312203776.0000 - r2: 0.8171 - val_loss: 11611829248.0000 - val_r2: 0.5748\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4308875776.0000 - r2: 0.8170 - val_loss: 10917797888.0000 - val_r2: 0.6002\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4311970304.0000 - r2: 0.8172 - val_loss: 10618851328.0000 - val_r2: 0.6107\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4308805632.0000 - r2: 0.8169 - val_loss: 11240960000.0000 - val_r2: 0.5884\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4313898496.0000 - r2: 0.8168 - val_loss: 10276703232.0000 - val_r2: 0.6234\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4307119616.0000 - r2: 0.8168 - val_loss: 11387730944.0000 - val_r2: 0.5829\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4308295168.0000 - r2: 0.8172 - val_loss: 10637644800.0000 - val_r2: 0.6104\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4310036480.0000 - r2: 0.8167 - val_loss: 10116751360.0000 - val_r2: 0.6300\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4308676096.0000 - r2: 0.8169 - val_loss: 10670757888.0000 - val_r2: 0.6090\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4308533248.0000 - r2: 0.8170 - val_loss: 10842461184.0000 - val_r2: 0.6028\n",
      "session cleared!\n",
      "\n",
      "ix 3 i 1\n",
      "updated temp_vec [0, 1, 1, 0, 1, 1, 0, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 0, 1, 1, 0, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83793453056.0000 - r2: -2.5432 - val_loss: 18048194560.0000 - val_r2: 0.3434\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9373725696.0000 - r2: 0.6031 - val_loss: 15110559744.0000 - val_r2: 0.4486\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8123372544.0000 - r2: 0.6557 - val_loss: 14178332672.0000 - val_r2: 0.4818\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7566222848.0000 - r2: 0.6794 - val_loss: 13477303296.0000 - val_r2: 0.5072\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7159742464.0000 - r2: 0.6964 - val_loss: 12849199104.0000 - val_r2: 0.5302\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6783286784.0000 - r2: 0.7126 - val_loss: 12940408832.0000 - val_r2: 0.5269\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6401275904.0000 - r2: 0.7285 - val_loss: 12239178752.0000 - val_r2: 0.5523\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6012796928.0000 - r2: 0.7449 - val_loss: 11940146176.0000 - val_r2: 0.5632\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5639873536.0000 - r2: 0.7607 - val_loss: 11951199232.0000 - val_r2: 0.5622\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5293108224.0000 - r2: 0.7755 - val_loss: 11446333440.0000 - val_r2: 0.5810\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4992667136.0000 - r2: 0.7878 - val_loss: 10794875904.0000 - val_r2: 0.6048\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4740267008.0000 - r2: 0.7988 - val_loss: 11057880064.0000 - val_r2: 0.5951\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4529575936.0000 - r2: 0.8079 - val_loss: 10541399040.0000 - val_r2: 0.6136\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4360370688.0000 - r2: 0.8146 - val_loss: 10824603648.0000 - val_r2: 0.6033\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4234121472.0000 - r2: 0.8200 - val_loss: 10053271552.0000 - val_r2: 0.6320\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4156019968.0000 - r2: 0.8235 - val_loss: 10532268032.0000 - val_r2: 0.6143\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4099003648.0000 - r2: 0.8260 - val_loss: 10434284544.0000 - val_r2: 0.6178\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4069341440.0000 - r2: 0.8270 - val_loss: 10520492032.0000 - val_r2: 0.6138\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4060891136.0000 - r2: 0.8276 - val_loss: 9632365568.0000 - val_r2: 0.6473\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4047939584.0000 - r2: 0.8278 - val_loss: 10133447680.0000 - val_r2: 0.6287\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4049009408.0000 - r2: 0.8279 - val_loss: 10390014976.0000 - val_r2: 0.6202\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4049967360.0000 - r2: 0.8277 - val_loss: 9948754944.0000 - val_r2: 0.6352\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4050358784.0000 - r2: 0.8274 - val_loss: 10603695104.0000 - val_r2: 0.6116\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4048640512.0000 - r2: 0.8279 - val_loss: 10257072128.0000 - val_r2: 0.6244\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4049286912.0000 - r2: 0.8279 - val_loss: 10626932736.0000 - val_r2: 0.6108\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4051057664.0000 - r2: 0.8279 - val_loss: 10358333440.0000 - val_r2: 0.6196\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4049014016.0000 - r2: 0.8279 - val_loss: 10292021248.0000 - val_r2: 0.6229\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4052834304.0000 - r2: 0.8277 - val_loss: 10523151360.0000 - val_r2: 0.6145\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4049313792.0000 - r2: 0.8276 - val_loss: 10045377536.0000 - val_r2: 0.6314\n",
      "session cleared!\n",
      "\n",
      "ix 4 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 0, 1, 0, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 0, 1, 0, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83919863808.0000 - r2: -2.5509 - val_loss: 17962620928.0000 - val_r2: 0.3475\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9392357376.0000 - r2: 0.6029 - val_loss: 15246248960.0000 - val_r2: 0.4438\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8161935872.0000 - r2: 0.6542 - val_loss: 13975411712.0000 - val_r2: 0.4902\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7620348416.0000 - r2: 0.6764 - val_loss: 13411568640.0000 - val_r2: 0.5093\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7236435456.0000 - r2: 0.6930 - val_loss: 12918425600.0000 - val_r2: 0.5273\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6884269568.0000 - r2: 0.7077 - val_loss: 13014245376.0000 - val_r2: 0.5238\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6531901440.0000 - r2: 0.7228 - val_loss: 12413325312.0000 - val_r2: 0.5461\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6184103936.0000 - r2: 0.7380 - val_loss: 11856643072.0000 - val_r2: 0.5655\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5850116096.0000 - r2: 0.7518 - val_loss: 11527526400.0000 - val_r2: 0.5778\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5545887744.0000 - r2: 0.7648 - val_loss: 12403202048.0000 - val_r2: 0.5464\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5289587200.0000 - r2: 0.7755 - val_loss: 11307867136.0000 - val_r2: 0.5856\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5061973504.0000 - r2: 0.7854 - val_loss: 11025552384.0000 - val_r2: 0.5965\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4872112640.0000 - r2: 0.7934 - val_loss: 10769132544.0000 - val_r2: 0.6051\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4711345152.0000 - r2: 0.8002 - val_loss: 10935745536.0000 - val_r2: 0.5994\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4587463168.0000 - r2: 0.8054 - val_loss: 10785999872.0000 - val_r2: 0.6052\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4499338752.0000 - r2: 0.8090 - val_loss: 10495132672.0000 - val_r2: 0.6158\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4436440576.0000 - r2: 0.8115 - val_loss: 11531452416.0000 - val_r2: 0.5778\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4400982528.0000 - r2: 0.8127 - val_loss: 10750526464.0000 - val_r2: 0.6056\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4377972224.0000 - r2: 0.8136 - val_loss: 10570318848.0000 - val_r2: 0.6126\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4371724800.0000 - r2: 0.8145 - val_loss: 9908303872.0000 - val_r2: 0.6371\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4366943232.0000 - r2: 0.8146 - val_loss: 10406959104.0000 - val_r2: 0.6180\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4368788480.0000 - r2: 0.8142 - val_loss: 10707963904.0000 - val_r2: 0.6074\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4365166592.0000 - r2: 0.8143 - val_loss: 9777769472.0000 - val_r2: 0.6414\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4360126976.0000 - r2: 0.8149 - val_loss: 10321369088.0000 - val_r2: 0.6219\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4360343040.0000 - r2: 0.8147 - val_loss: 10895797248.0000 - val_r2: 0.6005\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4365467136.0000 - r2: 0.8143 - val_loss: 10714351616.0000 - val_r2: 0.6068\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4364457984.0000 - r2: 0.8148 - val_loss: 10649474048.0000 - val_r2: 0.6092\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4361560576.0000 - r2: 0.8144 - val_loss: 11120081920.0000 - val_r2: 0.5922\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4364187648.0000 - r2: 0.8146 - val_loss: 11552462848.0000 - val_r2: 0.5762\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4362395136.0000 - r2: 0.8148 - val_loss: 10441269248.0000 - val_r2: 0.6173\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4358671360.0000 - r2: 0.8146 - val_loss: 10765799424.0000 - val_r2: 0.6049\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4365680128.0000 - r2: 0.8147 - val_loss: 10708961280.0000 - val_r2: 0.6072\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4361576960.0000 - r2: 0.8147 - val_loss: 10916336640.0000 - val_r2: 0.5998\n",
      "session cleared!\n",
      "\n",
      "ix 5 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 84371333120.0000 - r2: -2.5765 - val_loss: 18686388224.0000 - val_r2: 0.3210\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9524235264.0000 - r2: 0.5978 - val_loss: 15614382080.0000 - val_r2: 0.4297\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8317708800.0000 - r2: 0.6477 - val_loss: 14386483200.0000 - val_r2: 0.4747\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7794613760.0000 - r2: 0.6702 - val_loss: 14286009344.0000 - val_r2: 0.4778\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7435457024.0000 - r2: 0.6848 - val_loss: 13065964544.0000 - val_r2: 0.5215\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7098687488.0000 - r2: 0.6986 - val_loss: 13074604032.0000 - val_r2: 0.5221\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6747190784.0000 - r2: 0.7136 - val_loss: 12387640320.0000 - val_r2: 0.5471\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6366137856.0000 - r2: 0.7299 - val_loss: 12882551808.0000 - val_r2: 0.5285\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5968778752.0000 - r2: 0.7469 - val_loss: 11826561024.0000 - val_r2: 0.5670\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5569426432.0000 - r2: 0.7637 - val_loss: 10959800320.0000 - val_r2: 0.5992\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5186244608.0000 - r2: 0.7799 - val_loss: 11540880384.0000 - val_r2: 0.5779\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4862503424.0000 - r2: 0.7939 - val_loss: 10494723072.0000 - val_r2: 0.6166\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4591574528.0000 - r2: 0.8051 - val_loss: 11321662464.0000 - val_r2: 0.5855\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4368728064.0000 - r2: 0.8146 - val_loss: 10170118144.0000 - val_r2: 0.6278\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4213311488.0000 - r2: 0.8207 - val_loss: 10602686464.0000 - val_r2: 0.6118\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4092627456.0000 - r2: 0.8262 - val_loss: 9890996224.0000 - val_r2: 0.6384\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4022979328.0000 - r2: 0.8293 - val_loss: 11168140288.0000 - val_r2: 0.5907\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3979743744.0000 - r2: 0.8311 - val_loss: 10194356224.0000 - val_r2: 0.6265\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3951117824.0000 - r2: 0.8323 - val_loss: 9412401152.0000 - val_r2: 0.6554\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3937585408.0000 - r2: 0.8323 - val_loss: 9096219648.0000 - val_r2: 0.6667\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3917091584.0000 - r2: 0.8336 - val_loss: 10418855936.0000 - val_r2: 0.6179\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3904449024.0000 - r2: 0.8340 - val_loss: 10801277952.0000 - val_r2: 0.6040\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3897027584.0000 - r2: 0.8341 - val_loss: 9161884672.0000 - val_r2: 0.6650\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3889399552.0000 - r2: 0.8350 - val_loss: 9837389824.0000 - val_r2: 0.6399\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3879763712.0000 - r2: 0.8352 - val_loss: 10398830592.0000 - val_r2: 0.6179\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3868733952.0000 - r2: 0.8353 - val_loss: 9509779456.0000 - val_r2: 0.6515\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3862706432.0000 - r2: 0.8357 - val_loss: 10379034624.0000 - val_r2: 0.6195\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3851609344.0000 - r2: 0.8367 - val_loss: 10123697152.0000 - val_r2: 0.6290\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3848138752.0000 - r2: 0.8363 - val_loss: 11512753152.0000 - val_r2: 0.5780\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 3841390592.0000 - r2: 0.8365 - val_loss: 10372819968.0000 - val_r2: 0.6208\n",
      "new min loss: len 7, ix 5\n",
      "session cleared!\n",
      "\n",
      "ix 6 i 0\n",
      "ix 7 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 1, 0, 0, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 1, 0, 0, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 84016275456.0000 - r2: -2.5252 - val_loss: 18391066624.0000 - val_r2: 0.3316\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9567910912.0000 - r2: 0.5959 - val_loss: 15429454848.0000 - val_r2: 0.4366\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8401012224.0000 - r2: 0.6440 - val_loss: 13935760384.0000 - val_r2: 0.4904\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7935512064.0000 - r2: 0.6637 - val_loss: 13903556608.0000 - val_r2: 0.4920\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7624537600.0000 - r2: 0.6767 - val_loss: 13741676544.0000 - val_r2: 0.4976\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7341304832.0000 - r2: 0.6884 - val_loss: 12416688128.0000 - val_r2: 0.5457\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7053516288.0000 - r2: 0.7007 - val_loss: 12550067200.0000 - val_r2: 0.5412\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6744735232.0000 - r2: 0.7141 - val_loss: 12455897088.0000 - val_r2: 0.5434\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6420152832.0000 - r2: 0.7280 - val_loss: 12641503232.0000 - val_r2: 0.5372\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6093853696.0000 - r2: 0.7412 - val_loss: 11775325184.0000 - val_r2: 0.5688\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5769456128.0000 - r2: 0.7554 - val_loss: 11769500672.0000 - val_r2: 0.5693\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5462257152.0000 - r2: 0.7683 - val_loss: 11169593344.0000 - val_r2: 0.5913\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5171779072.0000 - r2: 0.7807 - val_loss: 11499299840.0000 - val_r2: 0.5790\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4929242624.0000 - r2: 0.7906 - val_loss: 10758926336.0000 - val_r2: 0.6058\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4728667648.0000 - r2: 0.7993 - val_loss: 10297697280.0000 - val_r2: 0.6226\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4588178432.0000 - r2: 0.8053 - val_loss: 10311399424.0000 - val_r2: 0.6227\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4504940544.0000 - r2: 0.8086 - val_loss: 10261923840.0000 - val_r2: 0.6243\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4457596416.0000 - r2: 0.8102 - val_loss: 11093683200.0000 - val_r2: 0.5934\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4431859712.0000 - r2: 0.8121 - val_loss: 10383995904.0000 - val_r2: 0.6192\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4411459072.0000 - r2: 0.8128 - val_loss: 9869056000.0000 - val_r2: 0.6380\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4394965504.0000 - r2: 0.8132 - val_loss: 11229720576.0000 - val_r2: 0.5885\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4385844736.0000 - r2: 0.8136 - val_loss: 10411809792.0000 - val_r2: 0.6182\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4379905024.0000 - r2: 0.8140 - val_loss: 9869601792.0000 - val_r2: 0.6379\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4369960960.0000 - r2: 0.8144 - val_loss: 10893921280.0000 - val_r2: 0.6007\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4360532992.0000 - r2: 0.8151 - val_loss: 9976648704.0000 - val_r2: 0.6343\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4356997120.0000 - r2: 0.8144 - val_loss: 10621778944.0000 - val_r2: 0.6110\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4350898176.0000 - r2: 0.8154 - val_loss: 10326710272.0000 - val_r2: 0.6211\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4344685056.0000 - r2: 0.8153 - val_loss: 10631309312.0000 - val_r2: 0.6103\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4336909312.0000 - r2: 0.8155 - val_loss: 10721864704.0000 - val_r2: 0.6074\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4333404672.0000 - r2: 0.8159 - val_loss: 9289379840.0000 - val_r2: 0.6597\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4335321088.0000 - r2: 0.8156 - val_loss: 10088204288.0000 - val_r2: 0.6301\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4319906816.0000 - r2: 0.8162 - val_loss: 10440757248.0000 - val_r2: 0.6173\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4322811904.0000 - r2: 0.8166 - val_loss: 9566310400.0000 - val_r2: 0.6494\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4320857088.0000 - r2: 0.8166 - val_loss: 10573116416.0000 - val_r2: 0.6119\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4311997952.0000 - r2: 0.8168 - val_loss: 10451291136.0000 - val_r2: 0.6169\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4311609344.0000 - r2: 0.8166 - val_loss: 9960503296.0000 - val_r2: 0.6350\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4312603136.0000 - r2: 0.8167 - val_loss: 10612739072.0000 - val_r2: 0.6116\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4306817536.0000 - r2: 0.8171 - val_loss: 9664379904.0000 - val_r2: 0.6460\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4303596544.0000 - r2: 0.8167 - val_loss: 10216649728.0000 - val_r2: 0.6261\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4298840064.0000 - r2: 0.8172 - val_loss: 9636554752.0000 - val_r2: 0.6470\n",
      "session cleared!\n",
      "\n",
      "ix 8 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 1, 0, 1, 0]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83800211456.0000 - r2: -2.5582 - val_loss: 18781927424.0000 - val_r2: 0.3168\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9752740864.0000 - r2: 0.5877 - val_loss: 15927121920.0000 - val_r2: 0.4181\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8676987904.0000 - r2: 0.6324 - val_loss: 14758786048.0000 - val_r2: 0.4610\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8298673664.0000 - r2: 0.6483 - val_loss: 14535880704.0000 - val_r2: 0.4688\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8087645696.0000 - r2: 0.6570 - val_loss: 13815581696.0000 - val_r2: 0.4949\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7928750080.0000 - r2: 0.6639 - val_loss: 14223835136.0000 - val_r2: 0.4787\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7786971136.0000 - r2: 0.6692 - val_loss: 14610836480.0000 - val_r2: 0.4661\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7657887744.0000 - r2: 0.6754 - val_loss: 13649122304.0000 - val_r2: 0.5011\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7539061248.0000 - r2: 0.6801 - val_loss: 13514673152.0000 - val_r2: 0.5052\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7442177024.0000 - r2: 0.6838 - val_loss: 13937095680.0000 - val_r2: 0.4893\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7350930944.0000 - r2: 0.6882 - val_loss: 13608912896.0000 - val_r2: 0.5015\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7285389824.0000 - r2: 0.6906 - val_loss: 13576272896.0000 - val_r2: 0.5034\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7239026176.0000 - r2: 0.6932 - val_loss: 14308842496.0000 - val_r2: 0.4764\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7205018112.0000 - r2: 0.6946 - val_loss: 14327917568.0000 - val_r2: 0.4761\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7181932032.0000 - r2: 0.6949 - val_loss: 14551773184.0000 - val_r2: 0.4672\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7172540928.0000 - r2: 0.6952 - val_loss: 13801374720.0000 - val_r2: 0.4958\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7149157888.0000 - r2: 0.6964 - val_loss: 13594110976.0000 - val_r2: 0.5031\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7138147328.0000 - r2: 0.6974 - val_loss: 13849418752.0000 - val_r2: 0.4925\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7135113728.0000 - r2: 0.6974 - val_loss: 13969518592.0000 - val_r2: 0.4891\n",
      "session cleared!\n",
      "\n",
      "1430.322092294693 seconds elapsed\n",
      "\n",
      "vec [0, 1, 1, 1, 1, 0, 0, 1, 1]\n",
      "ix 0 i 0\n",
      "ix 1 i 1\n",
      "updated temp_vec [0, 0, 1, 1, 1, 0, 0, 1, 1]\n",
      "going through feature_mask [0, 0, 1, 1, 1, 0, 0, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 90146463744.0000 - r2: -2.8009 - val_loss: 20036366336.0000 - val_r2: 0.2712\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 11000699904.0000 - r2: 0.5355 - val_loss: 17413464064.0000 - val_r2: 0.3650\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9935208448.0000 - r2: 0.5796 - val_loss: 16553443328.0000 - val_r2: 0.3966\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9372958720.0000 - r2: 0.6031 - val_loss: 15904566272.0000 - val_r2: 0.4199\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8921380864.0000 - r2: 0.6220 - val_loss: 15549448192.0000 - val_r2: 0.4328\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8473747456.0000 - r2: 0.6412 - val_loss: 14399237120.0000 - val_r2: 0.4739\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7999761408.0000 - r2: 0.6612 - val_loss: 14878332928.0000 - val_r2: 0.4573\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7486894592.0000 - r2: 0.6836 - val_loss: 13758944256.0000 - val_r2: 0.4974\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6955338752.0000 - r2: 0.7059 - val_loss: 13637035008.0000 - val_r2: 0.5016\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6426324480.0000 - r2: 0.7285 - val_loss: 12661112832.0000 - val_r2: 0.5379\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5946855424.0000 - r2: 0.7485 - val_loss: 12123574272.0000 - val_r2: 0.5566\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5550158336.0000 - r2: 0.7649 - val_loss: 12754508800.0000 - val_r2: 0.5340\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5214307840.0000 - r2: 0.7790 - val_loss: 11275670528.0000 - val_r2: 0.5879\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4978152960.0000 - r2: 0.7890 - val_loss: 11300332544.0000 - val_r2: 0.5869\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4810417152.0000 - r2: 0.7957 - val_loss: 11100369920.0000 - val_r2: 0.5934\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4700152320.0000 - r2: 0.8005 - val_loss: 11470564352.0000 - val_r2: 0.5804\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4636758016.0000 - r2: 0.8032 - val_loss: 12163402752.0000 - val_r2: 0.5546\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4604421632.0000 - r2: 0.8049 - val_loss: 10722771968.0000 - val_r2: 0.6077\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4575416832.0000 - r2: 0.8057 - val_loss: 11141793792.0000 - val_r2: 0.5921\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4564972544.0000 - r2: 0.8063 - val_loss: 10309777408.0000 - val_r2: 0.6235\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4556046848.0000 - r2: 0.8063 - val_loss: 10971065344.0000 - val_r2: 0.5985\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4538958848.0000 - r2: 0.8076 - val_loss: 11589285888.0000 - val_r2: 0.5760\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4531006976.0000 - r2: 0.8077 - val_loss: 10089864192.0000 - val_r2: 0.6310\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4520794624.0000 - r2: 0.8081 - val_loss: 11050857472.0000 - val_r2: 0.5953\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4514453504.0000 - r2: 0.8081 - val_loss: 11638915072.0000 - val_r2: 0.5733\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4506048512.0000 - r2: 0.8087 - val_loss: 11347329024.0000 - val_r2: 0.5848\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4500792320.0000 - r2: 0.8091 - val_loss: 11465971712.0000 - val_r2: 0.5801\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4491407872.0000 - r2: 0.8096 - val_loss: 11180872704.0000 - val_r2: 0.5904\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4486199296.0000 - r2: 0.8098 - val_loss: 11863561216.0000 - val_r2: 0.5658\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4481029120.0000 - r2: 0.8100 - val_loss: 11731342336.0000 - val_r2: 0.5699\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4476551168.0000 - r2: 0.8099 - val_loss: 11745348608.0000 - val_r2: 0.5699\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4472165376.0000 - r2: 0.8100 - val_loss: 10989080576.0000 - val_r2: 0.5980\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4464374272.0000 - r2: 0.8102 - val_loss: 11147214848.0000 - val_r2: 0.5922\n",
      "session cleared!\n",
      "\n",
      "ix 2 i 1\n",
      "updated temp_vec [0, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "going through feature_mask [0, 1, 0, 1, 1, 0, 0, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 116891172864.0000 - r2: -3.9633 - val_loss: 32765126656.0000 - val_r2: -0.1921\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 19675162624.0000 - r2: 0.1695 - val_loss: 27499612160.0000 - val_r2: -0.0028\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 17648375808.0000 - r2: 0.2538 - val_loss: 25799665664.0000 - val_r2: 0.0590\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 16380920832.0000 - r2: 0.3075 - val_loss: 24357488640.0000 - val_r2: 0.1116\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 15166766080.0000 - r2: 0.3579 - val_loss: 23280254976.0000 - val_r2: 0.1500\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 13816082432.0000 - r2: 0.4144 - val_loss: 21267165184.0000 - val_r2: 0.2228\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 12301122560.0000 - r2: 0.4795 - val_loss: 20389742592.0000 - val_r2: 0.2554\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 10646465536.0000 - r2: 0.5491 - val_loss: 17506809856.0000 - val_r2: 0.3603\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8990558208.0000 - r2: 0.6199 - val_loss: 15683751936.0000 - val_r2: 0.4267\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7500900864.0000 - r2: 0.6825 - val_loss: 14253642752.0000 - val_r2: 0.4791\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6317470208.0000 - r2: 0.7323 - val_loss: 13308520448.0000 - val_r2: 0.5132\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5493986304.0000 - r2: 0.7673 - val_loss: 12276319232.0000 - val_r2: 0.5518\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4986436096.0000 - r2: 0.7890 - val_loss: 12116660224.0000 - val_r2: 0.5561\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4706762240.0000 - r2: 0.8002 - val_loss: 11563707392.0000 - val_r2: 0.5766\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4568224768.0000 - r2: 0.8065 - val_loss: 11201124352.0000 - val_r2: 0.5899\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4510497792.0000 - r2: 0.8085 - val_loss: 10960939008.0000 - val_r2: 0.5989\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4488552960.0000 - r2: 0.8097 - val_loss: 10769509376.0000 - val_r2: 0.6062\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4481922048.0000 - r2: 0.8096 - val_loss: 11254085632.0000 - val_r2: 0.5882\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4481205248.0000 - r2: 0.8098 - val_loss: 11321387008.0000 - val_r2: 0.5853\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4480237056.0000 - r2: 0.8100 - val_loss: 10931162112.0000 - val_r2: 0.5999\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4480791552.0000 - r2: 0.8102 - val_loss: 11112827904.0000 - val_r2: 0.5938\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4479964160.0000 - r2: 0.8097 - val_loss: 10833495040.0000 - val_r2: 0.6031\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4481840640.0000 - r2: 0.8098 - val_loss: 10573914112.0000 - val_r2: 0.6132\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4478342656.0000 - r2: 0.8103 - val_loss: 11080835072.0000 - val_r2: 0.5948\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4480696320.0000 - r2: 0.8097 - val_loss: 10818692096.0000 - val_r2: 0.6041\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4478482944.0000 - r2: 0.8099 - val_loss: 11298949120.0000 - val_r2: 0.5862\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4478995968.0000 - r2: 0.8096 - val_loss: 10809484288.0000 - val_r2: 0.6042\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4481423360.0000 - r2: 0.8099 - val_loss: 11683550208.0000 - val_r2: 0.5723\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4483410944.0000 - r2: 0.8096 - val_loss: 10924393472.0000 - val_r2: 0.5997\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4481402880.0000 - r2: 0.8098 - val_loss: 10651063296.0000 - val_r2: 0.6093\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4477922816.0000 - r2: 0.8100 - val_loss: 11272416256.0000 - val_r2: 0.5873\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4485638656.0000 - r2: 0.8097 - val_loss: 11604481024.0000 - val_r2: 0.5754\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 1s 4ms/step - loss: 4481234432.0000 - r2: 0.8098 - val_loss: 10767513600.0000 - val_r2: 0.6055\n",
      "session cleared!\n",
      "\n",
      "ix 3 i 1\n",
      "updated temp_vec [0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83445964800.0000 - r2: -2.5651 - val_loss: 18529486848.0000 - val_r2: 0.3260\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9511372800.0000 - r2: 0.5978 - val_loss: 14915290112.0000 - val_r2: 0.4562\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8310459904.0000 - r2: 0.6474 - val_loss: 14332730368.0000 - val_r2: 0.4755\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7789040128.0000 - r2: 0.6697 - val_loss: 13660909568.0000 - val_r2: 0.5000\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7425052160.0000 - r2: 0.6853 - val_loss: 13460405248.0000 - val_r2: 0.5072\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7089691648.0000 - r2: 0.6991 - val_loss: 13135621120.0000 - val_r2: 0.5200\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6739990528.0000 - r2: 0.7139 - val_loss: 12851323904.0000 - val_r2: 0.5303\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6361536000.0000 - r2: 0.7298 - val_loss: 12379035648.0000 - val_r2: 0.5482\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5976451584.0000 - r2: 0.7461 - val_loss: 11680228352.0000 - val_r2: 0.5730\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5585273856.0000 - r2: 0.7629 - val_loss: 11845128192.0000 - val_r2: 0.5663\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5224573440.0000 - r2: 0.7786 - val_loss: 10919817216.0000 - val_r2: 0.6003\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4918484992.0000 - r2: 0.7913 - val_loss: 11347997696.0000 - val_r2: 0.5845\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4678230528.0000 - r2: 0.8017 - val_loss: 10301498368.0000 - val_r2: 0.6227\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4489434624.0000 - r2: 0.8095 - val_loss: 10686199808.0000 - val_r2: 0.6095\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4354114048.0000 - r2: 0.8152 - val_loss: 9950290944.0000 - val_r2: 0.6351\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4265401600.0000 - r2: 0.8191 - val_loss: 10603156480.0000 - val_r2: 0.6118\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4211734016.0000 - r2: 0.8213 - val_loss: 11062308864.0000 - val_r2: 0.5945\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4190034432.0000 - r2: 0.8219 - val_loss: 9842793472.0000 - val_r2: 0.6387\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4172589312.0000 - r2: 0.8228 - val_loss: 10065457152.0000 - val_r2: 0.6303\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4169281280.0000 - r2: 0.8227 - val_loss: 9880274944.0000 - val_r2: 0.6385\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4167497216.0000 - r2: 0.8231 - val_loss: 9671333888.0000 - val_r2: 0.6463\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4162997504.0000 - r2: 0.8231 - val_loss: 10075699200.0000 - val_r2: 0.6304\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4170805504.0000 - r2: 0.8229 - val_loss: 11176513536.0000 - val_r2: 0.5903\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4167296512.0000 - r2: 0.8230 - val_loss: 10819113984.0000 - val_r2: 0.6037\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4167415552.0000 - r2: 0.8229 - val_loss: 10736569344.0000 - val_r2: 0.6068\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4163687168.0000 - r2: 0.8227 - val_loss: 9845033984.0000 - val_r2: 0.6390\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4168500480.0000 - r2: 0.8230 - val_loss: 9951373312.0000 - val_r2: 0.6354\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4162771200.0000 - r2: 0.8231 - val_loss: 10022846464.0000 - val_r2: 0.6329\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4168816896.0000 - r2: 0.8227 - val_loss: 10115510272.0000 - val_r2: 0.6296\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4167518720.0000 - r2: 0.8232 - val_loss: 10195566592.0000 - val_r2: 0.6265\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4164757504.0000 - r2: 0.8231 - val_loss: 10151695360.0000 - val_r2: 0.6281\n",
      "session cleared!\n",
      "\n",
      "ix 4 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 0, 0, 0, 1, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 0, 0, 0, 1, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83587432448.0000 - r2: -2.5682 - val_loss: 18024263680.0000 - val_r2: 0.3447\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9521430528.0000 - r2: 0.5979 - val_loss: 15256175616.0000 - val_r2: 0.4436\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8330320896.0000 - r2: 0.6462 - val_loss: 13929097216.0000 - val_r2: 0.4921\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7829370880.0000 - r2: 0.6678 - val_loss: 13819139072.0000 - val_r2: 0.4949\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7484909056.0000 - r2: 0.6829 - val_loss: 13214672896.0000 - val_r2: 0.5165\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7168050176.0000 - r2: 0.6963 - val_loss: 13183775744.0000 - val_r2: 0.5172\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6842338816.0000 - r2: 0.7098 - val_loss: 12878156800.0000 - val_r2: 0.5294\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6502573056.0000 - r2: 0.7242 - val_loss: 12244828160.0000 - val_r2: 0.5520\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6152259584.0000 - r2: 0.7395 - val_loss: 11994029056.0000 - val_r2: 0.5611\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5812240384.0000 - r2: 0.7536 - val_loss: 11319104512.0000 - val_r2: 0.5859\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5496240640.0000 - r2: 0.7671 - val_loss: 11269211136.0000 - val_r2: 0.5869\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5228764672.0000 - r2: 0.7783 - val_loss: 11119448064.0000 - val_r2: 0.5929\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5000799232.0000 - r2: 0.7880 - val_loss: 10971732992.0000 - val_r2: 0.5983\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4822924288.0000 - r2: 0.7950 - val_loss: 10320931840.0000 - val_r2: 0.6218\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4694510592.0000 - r2: 0.8009 - val_loss: 11742513152.0000 - val_r2: 0.5698\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4594268672.0000 - r2: 0.8046 - val_loss: 10429423616.0000 - val_r2: 0.6180\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4533710848.0000 - r2: 0.8079 - val_loss: 10166188032.0000 - val_r2: 0.6273\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4498118656.0000 - r2: 0.8084 - val_loss: 10186775552.0000 - val_r2: 0.6266\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4484977152.0000 - r2: 0.8097 - val_loss: 11187607552.0000 - val_r2: 0.5899\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4480609280.0000 - r2: 0.8098 - val_loss: 11362035712.0000 - val_r2: 0.5840\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4477442560.0000 - r2: 0.8098 - val_loss: 9844955136.0000 - val_r2: 0.6396\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4471868928.0000 - r2: 0.8102 - val_loss: 10477588480.0000 - val_r2: 0.6152\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4476249088.0000 - r2: 0.8096 - val_loss: 10794662912.0000 - val_r2: 0.6040\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4468043776.0000 - r2: 0.8103 - val_loss: 10645787648.0000 - val_r2: 0.6103\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4470034432.0000 - r2: 0.8099 - val_loss: 10652855296.0000 - val_r2: 0.6097\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4470352896.0000 - r2: 0.8101 - val_loss: 11590002688.0000 - val_r2: 0.5757\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4473199616.0000 - r2: 0.8097 - val_loss: 10759284736.0000 - val_r2: 0.6054\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4470908416.0000 - r2: 0.8103 - val_loss: 10779720704.0000 - val_r2: 0.6055\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4465952256.0000 - r2: 0.8101 - val_loss: 11333297152.0000 - val_r2: 0.5846\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4464293888.0000 - r2: 0.8102 - val_loss: 11791102976.0000 - val_r2: 0.5680\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4467483136.0000 - r2: 0.8103 - val_loss: 10976492544.0000 - val_r2: 0.5978\n",
      "session cleared!\n",
      "\n",
      "ix 5 i 0\n",
      "ix 6 i 0\n",
      "ix 7 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 0, 0, 0, 1]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 83919060992.0000 - r2: -2.5849 - val_loss: 18284072960.0000 - val_r2: 0.3356\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9662164992.0000 - r2: 0.5910 - val_loss: 15442074624.0000 - val_r2: 0.4365\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8576373248.0000 - r2: 0.6368 - val_loss: 14692812800.0000 - val_r2: 0.4634\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8166273024.0000 - r2: 0.6538 - val_loss: 13890241536.0000 - val_r2: 0.4925\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7902879232.0000 - r2: 0.6645 - val_loss: 13521978368.0000 - val_r2: 0.5055\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7667283968.0000 - r2: 0.6742 - val_loss: 13762298880.0000 - val_r2: 0.4969\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7423485440.0000 - r2: 0.6851 - val_loss: 12987173888.0000 - val_r2: 0.5248\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7156075520.0000 - r2: 0.6966 - val_loss: 12902815744.0000 - val_r2: 0.5278\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6850831872.0000 - r2: 0.7095 - val_loss: 12137872384.0000 - val_r2: 0.5556\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6521731072.0000 - r2: 0.7237 - val_loss: 12192129024.0000 - val_r2: 0.5549\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 6154714112.0000 - r2: 0.7388 - val_loss: 11567261696.0000 - val_r2: 0.5768\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5780594176.0000 - r2: 0.7545 - val_loss: 11381020672.0000 - val_r2: 0.5837\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5430535168.0000 - r2: 0.7696 - val_loss: 11271649280.0000 - val_r2: 0.5874\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 5118891520.0000 - r2: 0.7827 - val_loss: 10581846016.0000 - val_r2: 0.6122\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4883044864.0000 - r2: 0.7929 - val_loss: 10912841728.0000 - val_r2: 0.6005\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4728729600.0000 - r2: 0.7993 - val_loss: 10773249024.0000 - val_r2: 0.6058\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4640606208.0000 - r2: 0.8031 - val_loss: 10141966336.0000 - val_r2: 0.6288\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4593913344.0000 - r2: 0.8049 - val_loss: 10069346304.0000 - val_r2: 0.6310\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4569959424.0000 - r2: 0.8058 - val_loss: 10964557824.0000 - val_r2: 0.5976\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4556332032.0000 - r2: 0.8064 - val_loss: 10632665088.0000 - val_r2: 0.6112\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4549309440.0000 - r2: 0.8065 - val_loss: 10354870272.0000 - val_r2: 0.6205\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4538954240.0000 - r2: 0.8070 - val_loss: 10774494208.0000 - val_r2: 0.6051\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4526308352.0000 - r2: 0.8077 - val_loss: 10070721536.0000 - val_r2: 0.6311\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4521906688.0000 - r2: 0.8081 - val_loss: 10349940736.0000 - val_r2: 0.6205\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4516073472.0000 - r2: 0.8079 - val_loss: 10316310528.0000 - val_r2: 0.6222\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4507717632.0000 - r2: 0.8083 - val_loss: 10518024192.0000 - val_r2: 0.6145\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4508352512.0000 - r2: 0.8085 - val_loss: 10909988864.0000 - val_r2: 0.5998\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4495156224.0000 - r2: 0.8089 - val_loss: 9977421824.0000 - val_r2: 0.6344\n",
      "Epoch 29/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4485533696.0000 - r2: 0.8096 - val_loss: 10453157888.0000 - val_r2: 0.6172\n",
      "Epoch 30/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4483357184.0000 - r2: 0.8093 - val_loss: 9841751040.0000 - val_r2: 0.6398\n",
      "Epoch 31/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4482566144.0000 - r2: 0.8098 - val_loss: 10003645440.0000 - val_r2: 0.6339\n",
      "Epoch 32/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4477719552.0000 - r2: 0.8099 - val_loss: 11017398272.0000 - val_r2: 0.5969\n",
      "Epoch 33/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4471456256.0000 - r2: 0.8099 - val_loss: 10240193536.0000 - val_r2: 0.6246\n",
      "Epoch 34/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4466287104.0000 - r2: 0.8103 - val_loss: 11026427904.0000 - val_r2: 0.5965\n",
      "Epoch 35/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4472948736.0000 - r2: 0.8100 - val_loss: 10700758016.0000 - val_r2: 0.6082\n",
      "Epoch 36/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4461091328.0000 - r2: 0.8104 - val_loss: 11285548032.0000 - val_r2: 0.5863\n",
      "Epoch 37/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4460231680.0000 - r2: 0.8104 - val_loss: 10695395328.0000 - val_r2: 0.6083\n",
      "Epoch 38/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4455735296.0000 - r2: 0.8106 - val_loss: 11162105856.0000 - val_r2: 0.5907\n",
      "Epoch 39/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4453740544.0000 - r2: 0.8111 - val_loss: 10038929408.0000 - val_r2: 0.6321\n",
      "Epoch 40/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 4456326656.0000 - r2: 0.8107 - val_loss: 10178365440.0000 - val_r2: 0.6269\n",
      "session cleared!\n",
      "\n",
      "ix 8 i 1\n",
      "updated temp_vec [0, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "going through feature_mask [0, 1, 1, 1, 1, 0, 0, 1, 0]\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 6), dtype=tf.float32, name=None), name='flatten/Reshape:0', description=\"created by layer 'flatten'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 8), dtype=tf.float32, name=None), name='flatten_2/Reshape:0', description=\"created by layer 'flatten_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 22), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_5/truediv:0', description=\"created by layer 'normalization_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_4/truediv:0', description=\"created by layer 'normalization_4'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_2/truediv:0', description=\"created by layer 'normalization_2'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_3/truediv:0', description=\"created by layer 'normalization_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization/truediv:0', description=\"created by layer 'normalization'\")\n",
      "Skipping KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='normalization_1/truediv:0', description=\"created by layer 'normalization_1'\")\n",
      "Epoch 1/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 84101398528.0000 - r2: -2.5705 - val_loss: 18293598208.0000 - val_r2: 0.3354\n",
      "Epoch 2/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 9882494976.0000 - r2: 0.5823 - val_loss: 16022545408.0000 - val_r2: 0.4157\n",
      "Epoch 3/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8856056832.0000 - r2: 0.6249 - val_loss: 14693538816.0000 - val_r2: 0.4627\n",
      "Epoch 4/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8520021504.0000 - r2: 0.6389 - val_loss: 14630214656.0000 - val_r2: 0.4650\n",
      "Epoch 5/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8358569984.0000 - r2: 0.6454 - val_loss: 14767358976.0000 - val_r2: 0.4604\n",
      "Epoch 6/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8247223296.0000 - r2: 0.6504 - val_loss: 14168797184.0000 - val_r2: 0.4815\n",
      "Epoch 7/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8146710528.0000 - r2: 0.6546 - val_loss: 14409947136.0000 - val_r2: 0.4737\n",
      "Epoch 8/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 8055885824.0000 - r2: 0.6584 - val_loss: 14793948160.0000 - val_r2: 0.4597\n",
      "Epoch 9/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7961962496.0000 - r2: 0.6619 - val_loss: 13875165184.0000 - val_r2: 0.4926\n",
      "Epoch 10/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7880573952.0000 - r2: 0.6657 - val_loss: 14354641920.0000 - val_r2: 0.4750\n",
      "Epoch 11/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7801496576.0000 - r2: 0.6692 - val_loss: 13507847168.0000 - val_r2: 0.5059\n",
      "Epoch 12/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7738386944.0000 - r2: 0.6719 - val_loss: 13805546496.0000 - val_r2: 0.4949\n",
      "Epoch 13/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7684825088.0000 - r2: 0.6741 - val_loss: 14179750912.0000 - val_r2: 0.4817\n",
      "Epoch 14/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7643566080.0000 - r2: 0.6752 - val_loss: 14339392512.0000 - val_r2: 0.4750\n",
      "Epoch 15/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7611014144.0000 - r2: 0.6770 - val_loss: 13233200128.0000 - val_r2: 0.5150\n",
      "Epoch 16/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7590297600.0000 - r2: 0.6772 - val_loss: 13288505344.0000 - val_r2: 0.5133\n",
      "Epoch 17/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7564120064.0000 - r2: 0.6789 - val_loss: 14236790784.0000 - val_r2: 0.4790\n",
      "Epoch 18/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7560748544.0000 - r2: 0.6796 - val_loss: 12738248704.0000 - val_r2: 0.5339\n",
      "Epoch 19/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7542912000.0000 - r2: 0.6796 - val_loss: 13042865152.0000 - val_r2: 0.5227\n",
      "Epoch 20/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7535606272.0000 - r2: 0.6804 - val_loss: 13319192576.0000 - val_r2: 0.5128\n",
      "Epoch 21/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7529489920.0000 - r2: 0.6807 - val_loss: 12842728448.0000 - val_r2: 0.5300\n",
      "Epoch 22/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7520656896.0000 - r2: 0.6810 - val_loss: 13797950464.0000 - val_r2: 0.4957\n",
      "Epoch 23/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7519001600.0000 - r2: 0.6807 - val_loss: 14456489984.0000 - val_r2: 0.4709\n",
      "Epoch 24/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7505835520.0000 - r2: 0.6816 - val_loss: 13557935104.0000 - val_r2: 0.5031\n",
      "Epoch 25/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7506881024.0000 - r2: 0.6817 - val_loss: 13660411904.0000 - val_r2: 0.4997\n",
      "Epoch 26/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7498037760.0000 - r2: 0.6810 - val_loss: 13664574464.0000 - val_r2: 0.4996\n",
      "Epoch 27/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7496159744.0000 - r2: 0.6815 - val_loss: 13964638208.0000 - val_r2: 0.4878\n",
      "Epoch 28/50\n",
      "342/342 [==============================] - 2s 4ms/step - loss: 7479407616.0000 - r2: 0.6828 - val_loss: 13562232832.0000 - val_r2: 0.5032\n",
      "session cleared!\n",
      "\n",
      "1786.8390173912048 seconds elapsed\n",
      "\n",
      "[[96733.95168191983, 101715.42168226016, 101727.67788561774, 97293.50958825568, 101198.9109822828, 98692.2178087006, 97383.84457393331, 98849.83876567529, 109803.25678230132], [101628.65485678731, 100155.23566943468, 97323.30088935538, 100410.18211317017, 96140.85666354341, 95777.07450115606, 99239.41926472564, 110106.16898248708], [101382.88360467955, 100135.26595560627, 98144.61558333192, 98882.60449644315, 95374.10365502787, 96381.42891657085, 116252.62643054564], [100448.3160237144, 102829.5391023416, 98342.94020416513, 99221.7472936251, 99205.59984194441, 112863.85029760415]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Implementation of recursive feature elimination algorithm for neural networks.\n",
    "\n",
    "Recursive feature elimination (RFE) is a feature selection method that removes unnecessary features from the inputs. \n",
    "It can also shed some insights on how much each feature contributes to the prediction task.\n",
    "\n",
    "The algorithm starts by removing one input feature whose removal leads to the minimum drop (or maximum improvement) in performance. \n",
    "- Given k features, to determine which of the k features will cause the minimum drop / maximum increase when that feature is removed, you will have to perform k experiments. After removing that feature, k-1 features will be left and you will have to perform k-1 experiments to determine the next feature to remove.\n",
    "    - In the case removing a feature leads to an improvement from the baseline (all features used), that feature is likely not useful since removing it actually helped the model to perform better.\n",
    "    - There will also be cases when all subsets with k-1 features do not do better than the baseline model. In that case, it is likely that all features are useful. You can get a sense of which are more useful than others by looking at the increase in error that occurs when the feature is removed.\n",
    "\n",
    "This procedure is repeated recursively on the reduced input set until the optimal number of input features is reached. \n",
    "- The feature removal goes on until either 1 feature is left, or the model performance does not improve from the previous best (e.g. when there are 7 features left, if none of the 7 experiments performed does better than the best performance of the model with 8 features, the RFE algorithm terminates).\n",
    "- The condition to stop the recursive process once all (k-1)-features models do worse than the best k-features model was added to make the algorithm terminate earlier so that you don't have to run too many iterations. Also, if the condition happens, then subsequent search is likely to be fruitless (it's always possible for the contrary to happen, but the chances are low and it's usually not worth the additional time).\n",
    "\n",
    "Each model should use a different subset of features and they are trained independently. There is no loading of weights from any previous models.\n",
    "\n",
    "\n",
    "In the code below, a boolean mask `vec` is used to keep track of which features to select during the iteration. \n",
    "\n",
    "You need to place your model training code into the 'train_model' function and have it return the validation loss.\n",
    "\n",
    "Look out for the comments labelled 'TODO'.\n",
    "\n",
    "\"\"\"\n",
    "import keras_tuner\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "num_features = 9\n",
    "\n",
    "vec = [1 for i in range(num_features)]\n",
    "best_loss = 1e15\n",
    "new_best_loss = 1e14\n",
    "which_iter = ''\n",
    "\n",
    "all_losses = [] # should be len 9,8,7,...\n",
    "\n",
    "\n",
    "def train_model(feature_mask):\n",
    "    \"\"\"\n",
    "    Given a boolean mask (feature_mask), select the features accordingly, train the model and return the validation loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_mask_string = ''.join([str(i) for i in feature_mask])\n",
    "\n",
    "    # TODO: define the input layer here (your code from Q2)\n",
    "\n",
    "    divisor = 2\n",
    "    hidden_units = 8\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.046185127256095915)\n",
    "    \n",
    "    month_num_categories = df[\"month\"].nunique()\n",
    "    flat_model_type_num_categories = df[\"flat_model_type\"].nunique()\n",
    "    storey_range_num_categories = df[\"storey_range\"].nunique()\n",
    "\n",
    "\n",
    "    #Integer categorical features\n",
    "    month_embedded = Q2_encode_categorical_feature(month, \"month\",train_ds, False, month_num_categories, divisor)\n",
    "    #String categorical features\n",
    "    flat_model_type_embedded = Q2_encode_categorical_feature(flat_model_type, \"flat_model_type\",train_ds, True, flat_model_type_num_categories, divisor)\n",
    "    storey_range_embedded = Q2_encode_categorical_feature(storey_range, \"storey_range\",train_ds, True, storey_range_num_categories, divisor)\n",
    "    \n",
    "    #Numerical features\n",
    "    dist_to_nearest_stn_encoded = encode_numerical_feature(dist_to_nearest_stn, \"dist_to_nearest_stn\", train_ds)\n",
    "    dist_to_dhoby_encoded = encode_numerical_feature(dist_to_dhoby, \"dist_to_dhoby\", train_ds)\n",
    "    degree_centrality_encoded = encode_numerical_feature(degree_centrality, \"degree_centrality\", train_ds)\n",
    "    eigenvector_centrality_encoded = encode_numerical_feature(eigenvector_centrality, \"eigenvector_centrality\", train_ds)\n",
    "    remaining_lease_year_encoded = encode_numerical_feature(remaining_lease_years, \"remaining_lease_years\", train_ds)\n",
    "    floor_area_sqm_encoded = encode_numerical_feature(floor_area_sqm,\"floor_area_sqm\", train_ds)\n",
    "    \n",
    "    all_features_input = [\n",
    "                        month_embedded,\n",
    "                        storey_range_embedded,\n",
    "                        flat_model_type_embedded,\n",
    "                        floor_area_sqm_encoded,\n",
    "                        remaining_lease_year_encoded,\n",
    "                        degree_centrality_encoded,\n",
    "                        eigenvector_centrality_encoded,\n",
    "                        dist_to_nearest_stn_encoded,\n",
    "                        dist_to_dhoby_encoded\n",
    "                    ]\n",
    "            \n",
    "    selected_inputs = []\n",
    "    print('going through feature_mask', feature_mask)\n",
    "    for i,j in zip(all_features_input, feature_mask):\n",
    "        if j == 1:\n",
    "            selected_inputs.append(i)\n",
    "            print(i)\n",
    "        else:\n",
    "            print('Skipping', i)\n",
    "\n",
    "    all_features = layers.concatenate(selected_inputs)\n",
    "    \n",
    "    \n",
    "    # TODO: Complete the rest of the architecture + training code and retrieve the training history\n",
    "    model_history = {}\n",
    "    # Model trained using the best hyperparameters\n",
    "    hidden_layer = layers.Dense(units=hidden_units, activation =\"linear\")(all_features)\n",
    "    output = layers.Dense(1, activation=\"linear\")(hidden_layer)\n",
    "    model = keras.Model(all_inputs, output)\n",
    "    model.compile(optimizer=optimizer, loss= \"mse\",metrics=[r2])\n",
    "\n",
    "    model_history[\"model\"] = model.fit(train_ds, epochs=50, validation_data = test_ds, verbose=1, callbacks=callback)\n",
    "    \n",
    "    val_loss_hx = square_roots(model_history[\"model\"].history[\"val_loss\"]) # NOTE: You can use RMSE if you find it easier to interpret.\n",
    "    val_loss_min = min(val_loss_hx)\n",
    "    \n",
    "    return val_loss_min\n",
    "\n",
    "\n",
    "## RFE starts here \n",
    "\n",
    "while sum(vec) > 0 and best_loss > new_best_loss:\n",
    "    \n",
    "    print('vec', vec)\n",
    "\n",
    "    best_loss = new_best_loss\n",
    "    new_min_loss_flag = False\n",
    "    \n",
    "    losses_from_same_vec = []\n",
    "    \n",
    "    for ix, i in enumerate(vec):\n",
    "        \n",
    "        print('ix', ix, 'i', i)\n",
    "        \n",
    "        if i == 0:\n",
    "            continue # if the feature is off, no need to do anything, go to next position\n",
    "        else:\n",
    "            temp_vec = vec[:]\n",
    "            temp_vec[ix] = 0 # turn off the feature\n",
    "            print('updated temp_vec', temp_vec)\n",
    "            \n",
    "            loss = train_model(temp_vec)\n",
    "            losses_from_same_vec.append(loss)\n",
    "            \n",
    "            if loss < new_best_loss:\n",
    "                new_best_loss = loss\n",
    "                which_iter = 'len ' + str(sum(vec)) + ', ix ' + str(ix)\n",
    "                print('new min loss:', which_iter)\n",
    "                new_min_loss_flag = True\n",
    "                min_loss_vec = temp_vec[:]\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "            print('session cleared!\\n')\n",
    "                \n",
    "    \n",
    "    all_losses.append(losses_from_same_vec)\n",
    "    \n",
    "    # After going through the vec once, update vec if new min loss    \n",
    "    if new_min_loss_flag:\n",
    "        vec = min_loss_vec\n",
    "    \n",
    "    # else case means no new min loss, the latter while loop condition will cause it to terminate \n",
    "    print(time.time() - start, 'seconds elapsed')\n",
    "    print()\n",
    "\n",
    "print(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['storey_range', 'flat_model_type', 'floor_area_sqm', 'remaining_lease_year', 'dist_to_nearest_stn', 'dist_to_dhoby']\n"
     ]
    }
   ],
   "source": [
    "best_feature_subset = []\n",
    "input = [\n",
    "                    \"month\",\n",
    "                    \"storey_range\",\n",
    "                    \"flat_model_type\",\n",
    "                    \"floor_area_sqm\",\n",
    "                    \"remaining_lease_year\",\n",
    "                    \"degree_centrality\",\n",
    "                    \"eigenvector_centrality\",\n",
    "                    \"dist_to_nearest_stn\",\n",
    "                    \"dist_to_dhoby\"\n",
    "                    ]\n",
    "\n",
    "for i in range(len(vec)):\n",
    "    if vec[i] == 1:\n",
    "        best_feature_subset.append(input[i])\n",
    "\n",
    "print(best_feature_subset)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best features\n",
    "**Best feature subset is  storey_range, flat_model_type, floor_area_sqm, remaining_lease_year, dist_to_nearest_stn and dist_to_dhoby**\n",
    "\n",
    "#### Compare these features and discuss if there is any concept shift\n",
    "\n",
    "**From the best feature subset on the new test set, it is similar to the best feature subset of the old test set. I do not think that there is a concept shift on the features(\"Month\" and \"Degree_centrality\").**\n",
    "\n",
    "**In fact, I believe it is due to the pandemic, rising inflation and interest rates, it has led to a change in consumers' behaviour. People are more prudent about buying high-value assets and such behaviour are not reflected sufficiently by the inputted features. Therefore, when the model runs its prediction on the recent datasets, it performs poorly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "#### Question 1\n",
    "##### 1. Neural Networks vs Traditional machine learning models\n",
    "\n",
    "**Neural network models are capabale of learning from unlabeled or unstructured data whereas traditional machine learning models generally learn to process structured data. In the machine learning pipeline. feature engineering comes right after data cleaning and visualization where the expertise of data scientists is required. However, in the neural network pipeline, there is no need for explict feature engineering in the deep learning pipepine. The neural network learns features from the data by itself and captures all non-linear relationships.**\n",
    "\n",
    "**Neural networks provide flexibility in the structure of inputs and outputs which machine learning lacks. They are capabale of capturing spatial and temporal relationships between features.**\n",
    "\n",
    "##### 2. Improvements to traditional machine learning models\n",
    "**Feature selection and feature engineering would be the best bet to improve the accuracy of traditional machine learning models. Feature engineering helps to extracts more information from existing data and these features may allow the model to have a higher ability to explain the variance in training data.**\n",
    "\n",
    "**Feature selection finds out the best subset of attributes which better explains the relationship of independent variables with target variable.**\n",
    "\n",
    "\n",
    "#### Question 2\n",
    "\n",
    "##### 1. Bayesian and Hyperband optimization\n",
    "\n",
    "**Bayesian Optimization builds a probability model of the objective function and uses it to select hyperparameters to evaluate in the true objective function.**\n",
    "\n",
    "**Hyperband is essentially just a grid search over the optimal allocation strategy. So at each individual trial the set of hyperparameters is chosen randomly. Hyperband is the extension of the Successive Halving algorithm.** \n",
    "\n",
    "**Random search tests hyperparameter sets at random, hence it runs the risk of missing the ideal set of hyperparameters and forgoing peak model performance. However, with Bayesian optimization method,the user do not have to incorporate randomness and risk missing the optimal solution. Although, Bayesian optimization does has its drawbacks as additional time is required to determine the next hyperparameters to evaluate based on the results of the previous iterations.**\n",
    "\n",
    "##### 2. Random search vs Grid search\n",
    "\n",
    "**Depending on the size of the hyperparameter search space and use case, the benefits of using grid search against random search varies.** \n",
    "\n",
    "**Using the brute force grid search method is simple and straightforward but an increase in the size of hyperparameter search space will result in an exponential rise in run time and computation.**\n",
    "\n",
    "**Random search method would reduce the computation time significantly but user might run the risk of missing the optimal case.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Question 3\n",
    "\n",
    "**Concept shift may have been the lead cause of model degradation, due to the pandemic, rising tensions between superpowers and high inflation rate, the consumers' behaviours has changed significantly over the past few years.**\n",
    "\n",
    "**Since the model is trained on past data, the model do not have the information on the current economical factors that have led to changes in housing prices.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e20b04dd985343179d8656fb1930f451adca8610422f13b5e4759b9499c407bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
